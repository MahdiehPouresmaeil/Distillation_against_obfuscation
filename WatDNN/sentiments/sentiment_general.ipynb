{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T09:26:30.586086276Z",
     "start_time": "2026-02-19T09:26:30.378692813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertConfig, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id =\"prajjwal1/bert-tiny\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2, use_safetensors=True )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(model)\n",
    "#"
   ],
   "id": "604f50daf72a3942",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in prajjwal1/bert-tiny. Should have a `model_type` key in its config.json.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfunctional\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mF\u001B[39;00m\n\u001B[32m     10\u001B[39m model_id =\u001B[33m\"\u001B[39m\u001B[33mprajjwal1/bert-tiny\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m model = \u001B[43mAutoModelForSequenceClassification\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_labels\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_safetensors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001B[32m     13\u001B[39m \u001B[38;5;28mprint\u001B[39m(model)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:319\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    316\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mquantization_config\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    317\u001B[39m     _ = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mquantization_config\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m config, kwargs = \u001B[43mAutoConfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    320\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    321\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_unused_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    322\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    323\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    324\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    325\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    326\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    328\u001B[39m \u001B[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001B[39;00m\n\u001B[32m    329\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwargs_orig.get(\u001B[33m\"\u001B[39m\u001B[33mtorch_dtype\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) == \u001B[33m\"\u001B[39m\u001B[33mauto\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1445\u001B[39m, in \u001B[36mAutoConfig.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[39m\n\u001B[32m   1433\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1434\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThe checkpoint you are trying to load has model type `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_dict[\u001B[33m'\u001B[39m\u001B[33mmodel_type\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m` \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1435\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mbut Transformers does not recognize this architecture. This could be because of an \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1441\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m`pip install git+https://github.com/huggingface/transformers.git`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1442\u001B[39m         )\n\u001B[32m   1443\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m1445\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1446\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized model in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1447\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mShould have a `model_type` key in its \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1448\u001B[39m )\n",
      "\u001B[31mValueError\u001B[39m: Unrecognized model in prajjwal1/bert-tiny. Should have a `model_type` key in its config.json."
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T12:18:01.611381569Z",
     "start_time": "2026-02-19T12:17:59.209670629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertConfig, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id =\"prajjwal1/bert-tiny\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2, use_safetensors=True )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(model_id, num_labels=2)\n",
    "model = BertForSequenceClassification.from_pretrained(model_id, config=config)\n",
    "print( model)\n",
    "\n"
   ],
   "id": "442045e2c85cfa8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/latim/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 39/39 [00:00<00:00, 1893.04it/s, Materializing param=bert.pooler.dense.weight]                              \n",
      "\u001B[1mBertForSequenceClassification LOAD REPORT\u001B[0m from: prajjwal1/bert-tiny\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "bert.embeddings.position_ids               | UNEXPECTED | \n",
      "cls.predictions.decoder.bias               | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "\n",
      "\u001B[3mNotes:\n",
      "- UNEXPECTED\u001B[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001B[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T13:36:49.859534772Z",
     "start_time": "2026-02-19T13:36:49.796352503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "            print(f\"  {name}: {param.shape}\")\n",
    "            if name == \"bert.encoder.layer.1.output.dense.weight\":\n",
    "                print(\"  This is the word embedding layer.\")\n",
    "\n",
    "print(\"*******************************\")\n",
    "for name, param in model.named_modules():\n",
    "            if name == \"bert.encoder.layer.1.output.dense\":\n",
    "                print(\"  This is the word embedding layer.\")\n",
    "            print(f\"  {name}: {param}\")\n",
    "            if name == \"bert.encoder.layer.1.output\":\n",
    "                print(\"  This is the word embedding layer.\")\n"
   ],
   "id": "347f2b4cbdfbf378",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bert.embeddings.word_embeddings.weight: torch.Size([30522, 128])\n",
      "  bert.embeddings.position_embeddings.weight: torch.Size([512, 128])\n",
      "  bert.embeddings.token_type_embeddings.weight: torch.Size([2, 128])\n",
      "  bert.embeddings.LayerNorm.weight: torch.Size([128])\n",
      "  bert.embeddings.LayerNorm.bias: torch.Size([128])\n",
      "  bert.encoder.layer.0.attention.self.query.weight: torch.Size([128, 128])\n",
      "  bert.encoder.layer.0.attention.self.query.bias: torch.Size([128])\n",
      "  bert.encoder.layer.0.attention.self.key.weight: torch.Size([128, 128])\n",
      "  bert.encoder.layer.0.attention.self.key.bias: torch.Size([128])\n",
      "  bert.encoder.layer.0.attention.self.value.weight: torch.Size([128, 128])\n",
      "  bert.encoder.layer.0.attention.self.value.bias: torch.Size([128])\n",
      "  bert.encoder.layer.0.attention.output.dense.weight: torch.Size([128, 128])\n",
      "  bert.encoder.layer.0.attention.output.dense.bias: torch.Size([128])\n",
      "  bert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([128])\n",
      "  bert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([128])\n",
      "  bert.encoder.layer.0.intermediate.dense.weight: torch.Size([512, 128])\n",
      "  bert.encoder.layer.0.intermediate.dense.bias: torch.Size([512])\n",
      "  bert.encoder.layer.0.output.dense.weight: torch.Size([128, 512])\n",
      "  bert.encoder.layer.0.output.dense.bias: torch.Size([128])\n",
      "  bert.encoder.layer.0.output.LayerNorm.weight: torch.Size([128])\n",
      "  bert.encoder.layer.0.output.LayerNorm.bias: torch.Size([128])\n",
      "  bert.encoder.layer.1.attention.self.query.weight: torch.Size([128, 128])\n",
      "  bert.encoder.layer.1.attention.self.query.bias: torch.Size([128])\n",
      "  bert.encoder.layer.1.attention.self.key.weight: torch.Size([128, 128])\n",
      "  bert.encoder.layer.1.attention.self.key.bias: torch.Size([128])\n",
      "  bert.encoder.layer.1.attention.self.value.weight: torch.Size([128, 128])\n",
      "  bert.encoder.layer.1.attention.self.value.bias: torch.Size([128])\n",
      "  bert.encoder.layer.1.attention.output.dense.weight: torch.Size([128, 128])\n",
      "  bert.encoder.layer.1.attention.output.dense.bias: torch.Size([128])\n",
      "  bert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([128])\n",
      "  bert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([128])\n",
      "  bert.encoder.layer.1.intermediate.dense.weight: torch.Size([512, 128])\n",
      "  bert.encoder.layer.1.intermediate.dense.bias: torch.Size([512])\n",
      "  bert.encoder.layer.1.output.dense.weight: torch.Size([128, 512])\n",
      "  This is the word embedding layer.\n",
      "  bert.encoder.layer.1.output.dense.bias: torch.Size([128])\n",
      "  bert.encoder.layer.1.output.LayerNorm.weight: torch.Size([128])\n",
      "  bert.encoder.layer.1.output.LayerNorm.bias: torch.Size([128])\n",
      "  bert.pooler.dense.weight: torch.Size([128, 128])\n",
      "  bert.pooler.dense.bias: torch.Size([128])\n",
      "  classifier.weight: torch.Size([2, 128])\n",
      "  classifier.bias: torch.Size([2])\n",
      "*******************************\n",
      "  : BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "  bert: BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 128)\n",
      "    (token_type_embeddings): Embedding(2, 128)\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-1): 2 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "  bert.embeddings: BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 128)\n",
      "  (token_type_embeddings): Embedding(2, 128)\n",
      "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "  bert.embeddings.word_embeddings: Embedding(30522, 128, padding_idx=0)\n",
      "  bert.embeddings.position_embeddings: Embedding(512, 128)\n",
      "  bert.embeddings.token_type_embeddings: Embedding(2, 128)\n",
      "  bert.embeddings.LayerNorm: LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  bert.embeddings.dropout: Dropout(p=0.1, inplace=False)\n",
      "  bert.encoder: BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-1): 2 x BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "  bert.encoder.layer: ModuleList(\n",
      "  (0-1): 2 x BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "  bert.encoder.layer.0: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  bert.encoder.layer.0.attention: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  bert.encoder.layer.0.attention.self: BertSelfAttention(\n",
      "  (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "  bert.encoder.layer.0.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  bert.encoder.layer.0.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  bert.encoder.layer.0.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  bert.encoder.layer.0.attention.self.dropout: Dropout(p=0.1, inplace=False)\n",
      "  bert.encoder.layer.0.attention.output: BertSelfOutput(\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "  bert.encoder.layer.0.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  bert.encoder.layer.0.attention.output.LayerNorm: LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  bert.encoder.layer.0.attention.output.dropout: Dropout(p=0.1, inplace=False)\n",
      "  bert.encoder.layer.0.intermediate: BertIntermediate(\n",
      "  (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  bert.encoder.layer.0.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  bert.encoder.layer.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  bert.encoder.layer.0.output: BertOutput(\n",
      "  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "  bert.encoder.layer.0.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  bert.encoder.layer.0.output.LayerNorm: LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  bert.encoder.layer.0.output.dropout: Dropout(p=0.1, inplace=False)\n",
      "  bert.encoder.layer.1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  bert.encoder.layer.1.attention: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "  bert.encoder.layer.1.attention.self: BertSelfAttention(\n",
      "  (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "  bert.encoder.layer.1.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  bert.encoder.layer.1.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  bert.encoder.layer.1.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  bert.encoder.layer.1.attention.self.dropout: Dropout(p=0.1, inplace=False)\n",
      "  bert.encoder.layer.1.attention.output: BertSelfOutput(\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "  bert.encoder.layer.1.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  bert.encoder.layer.1.attention.output.LayerNorm: LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  bert.encoder.layer.1.attention.output.dropout: Dropout(p=0.1, inplace=False)\n",
      "  bert.encoder.layer.1.intermediate: BertIntermediate(\n",
      "  (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "  bert.encoder.layer.1.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  bert.encoder.layer.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  bert.encoder.layer.1.output: BertOutput(\n",
      "  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "  This is the word embedding layer.\n",
      "  This is the word embedding layer.\n",
      "  bert.encoder.layer.1.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  bert.encoder.layer.1.output.LayerNorm: LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  bert.encoder.layer.1.output.dropout: Dropout(p=0.1, inplace=False)\n",
      "  bert.pooler: BertPooler(\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "  bert.pooler.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  bert.pooler.activation: Tanh()\n",
      "  dropout: Dropout(p=0.1, inplace=False)\n",
      "  classifier: Linear(in_features=128, out_features=2, bias=True)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T09:26:53.862166055Z",
     "start_time": "2026-02-19T09:26:52.321203101Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"glue\", \"sst2\")",
   "id": "705fd2d45f9cae49",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T09:27:06.724322163Z",
     "start_time": "2026-02-19T09:27:06.698327971Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset)",
   "id": "a055b36ef381e14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T09:27:30.496436496Z",
     "start_time": "2026-02-19T09:27:30.436856704Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset['train'][0])",
   "id": "5ed6241a53dd32d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T09:39:16.404727225Z",
     "start_time": "2026-02-19T09:39:16.343328477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "print(inspect.signature(model.forward))"
   ],
   "id": "189de7195bf75bff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(input_ids: torch.Tensor | None = None, attention_mask: torch.Tensor | None = None, token_type_ids: torch.Tensor | None = None, position_ids: torch.Tensor | None = None, inputs_embeds: torch.Tensor | None = None, labels: torch.Tensor | None = None, **kwargs: Unpack[transformers.utils.generic.TransformersKwargs]) -> tuple[torch.Tensor] | transformers.modeling_outputs.SequenceClassifierOutput\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T09:39:59.892385266Z",
     "start_time": "2026-02-19T09:39:59.863525393Z"
    }
   },
   "cell_type": "code",
   "source": "print(model.config)",
   "id": "26ddd99574f4afdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"add_cross_attention\": false,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"5.2.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eee9443adb3a48d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

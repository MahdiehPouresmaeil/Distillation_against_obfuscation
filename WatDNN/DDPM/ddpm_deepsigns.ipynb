{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T12:47:37.871048382Z",
     "start_time": "2026-02-16T12:47:36.368024675Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "\n",
    "# --- Simple GMM Implementation ---\n",
    "\n",
    "class SimpleGMM:\n",
    "    \"\"\"Simple Gaussian Mixture Model using K-means initialization.\"\"\"\n",
    "    def __init__(self, n_components, n_features, n_iter=100):\n",
    "        self.n_components = n_components\n",
    "        self.n_features = n_features\n",
    "        self.n_iter = n_iter\n",
    "        self.mu = None  # [n_components, n_features]\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit GMM using K-means.\"\"\"\n",
    "        device = X.device\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Initialize centroids randomly\n",
    "        indices = torch.randperm(n_samples)[:self.n_components]\n",
    "        self.mu = X[indices].clone()\n",
    "\n",
    "        # K-means iterations\n",
    "        for _ in range(self.n_iter):\n",
    "            # Assign samples to nearest centroid\n",
    "            distances = torch.cdist(X, self.mu)\n",
    "            assignments = torch.argmin(distances, dim=1)\n",
    "\n",
    "            # Update centroids\n",
    "            new_mu = torch.zeros_like(self.mu)\n",
    "            for k in range(self.n_components):\n",
    "                mask = assignments == k\n",
    "                if mask.sum() > 0:\n",
    "                    new_mu[k] = X[mask].mean(dim=0)\n",
    "                else:\n",
    "                    new_mu[k] = self.mu[k]\n",
    "            self.mu = new_mu\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict cluster assignments.\"\"\"\n",
    "        distances = torch.cdist(X, self.mu)\n",
    "        return torch.argmin(distances, dim=1)\n",
    "\n",
    "\n",
    "# --- DeepSigns Module ---\n",
    "\n",
    "class DeepSignsModule(nn.Module):\n",
    "    \"\"\"Learnable GMM means that project to watermark space.\"\"\"\n",
    "    def __init__(self, gmm_mu):\n",
    "        super().__init__()\n",
    "        self.var_param = nn.Parameter(gmm_mu.clone(), requires_grad=True)\n",
    "\n",
    "    def forward(self, matrix_a):\n",
    "        matrix_g = torch.sigmoid(self.var_param @ matrix_a)\n",
    "        return matrix_g\n",
    "\n",
    "\n",
    "# --- Feature Hook ---\n",
    "\n",
    "class FeatureHook:\n",
    "    \"\"\"Hook to capture intermediate activations.\"\"\"\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.features = None\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "# --- Classe Principale DeepSigns DDPM ---\n",
    "\n",
    "class DeepSignsDDPM:\n",
    "    def __init__(self, model_id, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.model_id = model_id\n",
    "\n",
    "        # Chargement du modele\n",
    "        self.pipeline = DDPMPipeline.from_pretrained(model_id)\n",
    "        self.unet = self.pipeline.unet.to(device)\n",
    "        self.scheduler = self.pipeline.scheduler\n",
    "\n",
    "        # Configuration par defaut\n",
    "        self.config = {\n",
    "            \"layer_name\": \"mid_block.resnets.0.conv1\",  # Couche cible\n",
    "            \"watermark_len\": 64,\n",
    "            \"n_components\": 10,      # Nombre de clusters GMM\n",
    "            \"nb_wat_classes\": 3,     # Nombre de classes porteuses\n",
    "            \"trigger_size\": 256,     # Taille du trigger set\n",
    "            \"lr\": 1e-4,\n",
    "            \"lr_ds\": 1e-3,           # Learning rate for DeepSigns module\n",
    "            \"lambda_1\": 0.1,         # GMM loss weight\n",
    "            \"lambda_2\": 1.0,         # Watermark loss weight\n",
    "            \"epochs\": 5,\n",
    "        }\n",
    "\n",
    "        self.saved_keys = {}\n",
    "\n",
    "    def _get_layer(self, model, layer_name):\n",
    "        \"\"\"Navigate to a layer by its path.\"\"\"\n",
    "        layer = model\n",
    "        for part in layer_name.split('.'):\n",
    "            if part.isdigit():\n",
    "                layer = layer[int(part)]\n",
    "            else:\n",
    "                layer = getattr(layer, part)\n",
    "        return layer\n",
    "\n",
    "    def _extract_activations(self, model, dataloader, max_samples=1000):\n",
    "        \"\"\"\n",
    "        Extract activations from the target layer for GMM fitting.\n",
    "        \"\"\"\n",
    "        target_layer = self._get_layer(model, self.config[\"layer_name\"])\n",
    "        hook = FeatureHook(target_layer)\n",
    "\n",
    "        activations = []\n",
    "        inputs_list = []\n",
    "        timesteps_list = []\n",
    "\n",
    "        model.eval()\n",
    "        n_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for clean_images, _ in dataloader:\n",
    "                if n_samples >= max_samples:\n",
    "                    break\n",
    "\n",
    "                clean_images = clean_images.to(self.device)\n",
    "                bs = clean_images.shape[0]\n",
    "\n",
    "                noise = torch.randn_like(clean_images).to(self.device)\n",
    "                timesteps = torch.randint(0, self.scheduler.config.num_train_timesteps, (bs,), device=self.device).long()\n",
    "                noisy_images = self.scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "                _ = model(noisy_images, timesteps)\n",
    "\n",
    "                # Global average pooling and flatten\n",
    "                act = hook.features\n",
    "                if len(act.shape) == 4:\n",
    "                    act = act.mean(dim=(2, 3))  # [B, C]\n",
    "                activations.append(act.cpu())\n",
    "                inputs_list.append(noisy_images.cpu())\n",
    "                timesteps_list.append(timesteps.cpu())\n",
    "\n",
    "                n_samples += bs\n",
    "\n",
    "        hook.close()\n",
    "\n",
    "        activations = torch.cat(activations, dim=0)[:max_samples]\n",
    "        inputs_list = torch.cat(inputs_list, dim=0)[:max_samples]\n",
    "        timesteps_list = torch.cat(timesteps_list, dim=0)[:max_samples]\n",
    "\n",
    "        return activations.to(self.device), inputs_list.to(self.device), timesteps_list.to(self.device)\n",
    "\n",
    "    def _mu_loss(self, act, mu, mu_bar, watermarked_classes, y_key):\n",
    "        \"\"\"\n",
    "        Compute GMM loss:\n",
    "        - Minimize distance between carrier class means and statistical means\n",
    "        - Maximize distance between carrier and non-carrier means\n",
    "        \"\"\"\n",
    "        # Compute statistical means for each watermarked class\n",
    "        stat_means = torch.stack([\n",
    "            act[y_key == t].mean(dim=0) if (y_key == t).sum() > 0 else mu[i]\n",
    "            for i, t in enumerate(watermarked_classes)\n",
    "        ])\n",
    "\n",
    "        # Loss to approach GMM means to statistical means\n",
    "        gmm_loss = F.mse_loss(stat_means, mu, reduction='sum')\n",
    "\n",
    "        # Loss to separate carrier and non-carrier means\n",
    "        if len(mu_bar) > 0:\n",
    "            sep_loss = F.mse_loss(\n",
    "                mu.unsqueeze(1).expand(-1, len(mu_bar), -1),\n",
    "                mu_bar.unsqueeze(0).expand(len(mu), -1, -1),\n",
    "                reduction='mean'\n",
    "            )\n",
    "        else:\n",
    "            sep_loss = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        return gmm_loss, sep_loss\n",
    "\n",
    "    def embed(self, dataloader):\n",
    "        \"\"\"\n",
    "        Incorpore la marque DeepSigns pendant le finetuning.\n",
    "        Utilise GMM pour identifier les classes porteuses.\n",
    "        \"\"\"\n",
    "        print(f\"--- Demarrage Embedding DeepSigns ({self.config['layer_name']}) ---\")\n",
    "\n",
    "        watermarked_unet = self.unet\n",
    "\n",
    "        # 1. Extract activations for GMM\n",
    "        print(\"Extracting activations for GMM...\")\n",
    "        activations, trigger_inputs, trigger_timesteps = self._extract_activations(\n",
    "            watermarked_unet, dataloader, max_samples=self.config[\"trigger_size\"]\n",
    "        )\n",
    "        n_features = activations.shape[1]\n",
    "        print(f\"Activation shape: {activations.shape}\")\n",
    "\n",
    "        # 2. Fit GMM\n",
    "        print(\"Fitting GMM...\")\n",
    "        gmm = SimpleGMM(self.config[\"n_components\"], n_features)\n",
    "        gmm.fit(activations)\n",
    "\n",
    "        # 3. Select watermarked classes (carrier classes)\n",
    "        y_gmm = gmm.predict(activations)\n",
    "        unique_classes = torch.unique(y_gmm).tolist()\n",
    "        n_wat = min(self.config[\"nb_wat_classes\"], len(unique_classes))\n",
    "        watermarked_classes = torch.tensor(random.sample(unique_classes, n_wat), device=self.device)\n",
    "        print(f\"Watermarked classes: {watermarked_classes.tolist()}\")\n",
    "\n",
    "        # 4. Create trigger set (samples from watermarked classes)\n",
    "        trigger_mask = torch.isin(y_gmm, watermarked_classes.cpu())\n",
    "        x_key = trigger_inputs[trigger_mask]\n",
    "        t_key = trigger_timesteps[trigger_mask]\n",
    "        y_key = y_gmm[trigger_mask].to(self.device)\n",
    "        print(f\"Trigger set size: {len(x_key)}\")\n",
    "\n",
    "        if len(x_key) == 0:\n",
    "            print(\"Warning: Empty trigger set, using all samples\")\n",
    "            x_key = trigger_inputs\n",
    "            t_key = trigger_timesteps\n",
    "            y_key = y_gmm.to(self.device)\n",
    "\n",
    "        # 5. Generate watermark and matrix_a\n",
    "        watermark = torch.randint(0, 2, (n_wat, self.config[\"watermark_len\"])).float().to(self.device)\n",
    "        matrix_a = torch.randn(n_features, self.config[\"watermark_len\"]).to(self.device)\n",
    "        print(f\"Watermark shape: {watermark.shape}\")\n",
    "\n",
    "        # 6. Initialize DeepSigns module with GMM means of carrier classes\n",
    "        mu_carriers = gmm.mu[watermarked_classes.cpu()].to(self.device)\n",
    "        deepsigns_module = DeepSignsModule(mu_carriers).to(self.device)\n",
    "\n",
    "        # Non-carrier means (frozen)\n",
    "        non_carrier_idx = [i for i in range(self.config[\"n_components\"]) if i not in watermarked_classes.tolist()]\n",
    "        mu_bar = gmm.mu[non_carrier_idx].to(self.device).detach()\n",
    "\n",
    "        # 7. Optimizer\n",
    "        watermarked_unet.train()\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': watermarked_unet.parameters(), 'lr': self.config[\"lr\"]},\n",
    "            {'params': deepsigns_module.parameters(), 'lr': self.config[\"lr_ds\"]}\n",
    "        ])\n",
    "\n",
    "        mse_loss = nn.MSELoss()\n",
    "        bce_loss = nn.BCELoss(reduction='sum')\n",
    "\n",
    "        # Hook for activation extraction during training\n",
    "        target_layer = self._get_layer(watermarked_unet, self.config[\"layer_name\"])\n",
    "\n",
    "        # 8. Training loop\n",
    "        for epoch in range(self.config[\"epochs\"]):\n",
    "            pbar = tqdm(dataloader)\n",
    "            for clean_images, _ in pbar:\n",
    "                clean_images = clean_images.to(self.device)\n",
    "                bs = clean_images.shape[0]\n",
    "\n",
    "                # A. Main task loss\n",
    "                noise = torch.randn_like(clean_images).to(self.device)\n",
    "                timesteps = torch.randint(0, self.scheduler.config.num_train_timesteps, (bs,), device=self.device).long()\n",
    "                noisy_images = self.scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                noise_pred = watermarked_unet(noisy_images, timesteps).sample\n",
    "                l_main = mse_loss(noise_pred, noise)\n",
    "\n",
    "                # B. Extract activations from trigger set\n",
    "                hook = FeatureHook(target_layer)\n",
    "                _ = watermarked_unet(x_key, t_key)\n",
    "                act = hook.features\n",
    "                if len(act.shape) == 4:\n",
    "                    act = act.mean(dim=(2, 3))\n",
    "                hook.close()\n",
    "\n",
    "                # C. GMM Loss\n",
    "                mu_dp = deepsigns_module.var_param\n",
    "                gmm_loss, sep_loss = self._mu_loss(act, mu_dp, mu_bar, watermarked_classes, y_key)\n",
    "                l_mu = gmm_loss - sep_loss\n",
    "\n",
    "                # D. Watermark Loss\n",
    "                matrix_g = deepsigns_module(matrix_a)\n",
    "                l_wat = bce_loss(matrix_g, watermark)\n",
    "\n",
    "                # Total loss\n",
    "                l_total = l_main + self.config[\"lambda_1\"] * l_mu + self.config[\"lambda_2\"] * l_wat\n",
    "\n",
    "                l_total.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Metrics\n",
    "                ber = self._compute_ber(matrix_g, watermark)\n",
    "                pbar.set_description(\n",
    "                    f\"Epoch {epoch+1} | L_Main: {l_main:.3f} | L_Mu: {l_mu:.3f} | L_Wat: {l_wat:.3f} | BER: {ber:.2f}\"\n",
    "                )\n",
    "\n",
    "                if ber == 0.0 and l_wat.item() < 0.1:\n",
    "                    print(\"Convergence atteinte !\")\n",
    "                    break\n",
    "            if ber == 0.0:\n",
    "                break\n",
    "\n",
    "        # Save keys\n",
    "        self.saved_keys = {\n",
    "            \"watermark\": watermark,\n",
    "            \"matrix_a\": matrix_a,\n",
    "            \"watermarked_classes\": watermarked_classes,\n",
    "            \"watermarked_unet\": watermarked_unet,\n",
    "            \"deepsigns_module\": deepsigns_module,\n",
    "            \"x_key\": x_key,\n",
    "            \"t_key\": t_key,\n",
    "            \"y_key\": y_key,\n",
    "        }\n",
    "        return watermarked_unet\n",
    "\n",
    "    def extract(self, suspect_unet=None):\n",
    "        \"\"\"\n",
    "        Extrait la marque d'un modele suspect en utilisant les moyennes statistiques.\n",
    "        \"\"\"\n",
    "        if suspect_unet is None:\n",
    "            suspect_unet = self.saved_keys[\"watermarked_unet\"]\n",
    "\n",
    "        watermark = self.saved_keys[\"watermark\"]\n",
    "        matrix_a = self.saved_keys[\"matrix_a\"]\n",
    "        watermarked_classes = self.saved_keys[\"watermarked_classes\"]\n",
    "        x_key = self.saved_keys[\"x_key\"]\n",
    "        t_key = self.saved_keys[\"t_key\"]\n",
    "        y_key = self.saved_keys[\"y_key\"]\n",
    "\n",
    "        # Extract activations\n",
    "        target_layer = self._get_layer(suspect_unet, self.config[\"layer_name\"])\n",
    "        hook = FeatureHook(target_layer)\n",
    "\n",
    "        suspect_unet.eval()\n",
    "        with torch.no_grad():\n",
    "            _ = suspect_unet(x_key, t_key)\n",
    "            act = hook.features\n",
    "            if len(act.shape) == 4:\n",
    "                act = act.mean(dim=(2, 3))\n",
    "        hook.close()\n",
    "\n",
    "        # Compute statistical means for each watermarked class\n",
    "        mu_ext = torch.stack([\n",
    "            act[y_key == t].mean(dim=0) if (y_key == t).sum() > 0 else torch.zeros(act.shape[1], device=self.device)\n",
    "            for t in watermarked_classes\n",
    "        ])\n",
    "\n",
    "        # Project through matrix_a\n",
    "        g_ext = torch.sigmoid(mu_ext @ matrix_a)\n",
    "        ber = self._compute_ber(g_ext, watermark)\n",
    "\n",
    "        print(f\"BER Extrait : {ber:.2f}\")\n",
    "        return ber, g_ext\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_ber(pred, target):\n",
    "        return ((pred > 0.5).float() != target).float().mean().item()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/latim/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "embedding_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T12:47:40.947574227Z",
     "start_time": "2026-02-16T12:47:37.872970457Z"
    }
   },
   "source": [
    "# --- EXEMPLE D'EXECUTION ---\n",
    "\n",
    "# 1. Setup Data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 2. Embedding DeepSigns\n",
    "deepsigns_defense = DeepSignsDDPM(\"google/ddpm-cifar10-32\")\n",
    "watermarked_model = deepsigns_defense.embed(dataloader)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/latim/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80: Error no file named diffusion_pytorch_model.safetensors found in directory /home/latim/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Demarrage Embedding DeepSigns (mid_block.resnets.0.conv1) ---\n",
      "Extracting activations for GMM...\n",
      "Activation shape: torch.Size([256, 256])\n",
      "Fitting GMM...\n",
      "Watermarked classes: [2, 7, 8]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got test_elements is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_isin_Tensor_Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# 2. Embedding DeepSigns\u001B[39;00m\n\u001B[32m      9\u001B[39m deepsigns_defense = DeepSignsDDPM(\u001B[33m\"\u001B[39m\u001B[33mgoogle/ddpm-cifar10-32\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m watermarked_model = \u001B[43mdeepsigns_defense\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 224\u001B[39m, in \u001B[36mDeepSignsDDPM.embed\u001B[39m\u001B[34m(self, dataloader)\u001B[39m\n\u001B[32m    221\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mWatermarked classes: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwatermarked_classes.tolist()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    223\u001B[39m \u001B[38;5;66;03m# 4. Create trigger set (samples from watermarked classes)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m trigger_mask = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43misin\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_gmm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwatermarked_classes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    225\u001B[39m x_key = trigger_inputs[trigger_mask]\n\u001B[32m    226\u001B[39m t_key = trigger_timesteps[trigger_mask]\n",
      "\u001B[31mRuntimeError\u001B[39m: Expected all tensors to be on the same device, but got test_elements is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_isin_Tensor_Tensor)"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "extraction_cell",
   "metadata": {},
   "source": [
    "# 3. Extraction (Test immediat)\n",
    "ber, _ = deepsigns_defense.extract(watermarked_model)\n",
    "print(f\"\\nResultat final - BER: {ber:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "distillation_attack_cell",
   "metadata": {},
   "source": [
    "# --- Fonction de Distillation (Attaque) ---\n",
    "\n",
    "def run_distillation_attack_deepsigns(ds_obj, dataloader, epochs=5, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Tente de transferer la fonctionnalite du modele DeepSigns vers un modele vierge.\n",
    "    Verifie si la marque (basee sur les moyennes GMM) survit.\n",
    "    \"\"\"\n",
    "    device = ds_obj.device\n",
    "\n",
    "    # 1. Teacher (Gele)\n",
    "    teacher_unet = ds_obj.saved_keys[\"watermarked_unet\"]\n",
    "    teacher_unet.eval()\n",
    "    for p in teacher_unet.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 2. Student (Vierge - Meme architecture)\n",
    "    print(\"\\n--- Initialisation du Student ---\")\n",
    "    student_pipeline = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "    student_unet = student_pipeline.unet.to(device)\n",
    "    student_unet.train()\n",
    "\n",
    "    teacher_ber, _ = ds_obj.extract(teacher_unet)\n",
    "    student_ber, _ = ds_obj.extract(student_unet)\n",
    "    print(f\"[Check] BER Teacher: {teacher_ber:.2f}\")\n",
    "    print(f\"[Check] BER Student (Avant): {student_ber:.2f}\")\n",
    "\n",
    "    optimizer = AdamW(student_unet.parameters(), lr=lr)\n",
    "    noise_scheduler = ds_obj.scheduler\n",
    "    history = {\"loss\": [], \"ber\": []}\n",
    "\n",
    "    print(f\"\\n--- Distillation DeepSigns ({epochs} epochs) ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for clean_images, _ in pbar:\n",
    "            clean_images = clean_images.to(device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            noise = torch.randn_like(clean_images).to(device)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=device).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                target_pred = teacher_unet(noisy_images, timesteps).sample\n",
    "\n",
    "            student_pred = student_unet(noisy_images, timesteps).sample\n",
    "\n",
    "            loss = F.mse_loss(student_pred, target_pred)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix(Loss=loss.item())\n",
    "\n",
    "        current_ber, g_ext = ds_obj.extract(student_unet)\n",
    "        history[\"ber\"].append(current_ber)\n",
    "        history[\"loss\"].append(running_loss / len(dataloader))\n",
    "\n",
    "        err_wat = nn.BCELoss()(g_ext, ds_obj.saved_keys[\"watermark\"]).item()\n",
    "        print(f\"Fin Epoch {epoch+1} | Loss: {history['loss'][-1]:.4f} | BER Student: {current_ber:.2f} | err_wat: {err_wat:.4f}\")\n",
    "\n",
    "    return student_unet, history\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "run_attack_cell",
   "metadata": {},
   "source": [
    "# 4. Attaque par Distillation\n",
    "student_res, stats = run_distillation_attack_deepsigns(deepsigns_defense, dataloader, epochs=100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "plot_results_cell",
   "metadata": {},
   "source": [
    "# 5. Visualisation des resultats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(stats[\"loss\"])\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Distillation Loss\")\n",
    "\n",
    "ax2.plot(stats[\"ber\"])\n",
    "ax2.axhline(y=0.5, color='r', linestyle='--', label='Random (0.5)')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"BER\")\n",
    "ax2.set_title(\"BER Evolution During Distillation\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T13:47:24.383940319Z",
     "start_time": "2026-02-18T13:47:22.882010147Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from torch.optim import AdamW\n",
    "import hmac\n",
    "import hashlib\n",
    "\n",
    "# --- HufuNet Autoencoder ---\n",
    "\n",
    "class HufuEncoder(nn.Module):\n",
    "    \"\"\"Encoder part of HufuNet autoencoder.\"\"\"\n",
    "    def __init__(self, in_channels=3, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, latent_dim, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class HufuDecoder(nn.Module):\n",
    "    \"\"\"Decoder part of HufuNet autoencoder.\"\"\"\n",
    "    def __init__(self, in_channels=3, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, in_channels, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class HufuAutoencoder(nn.Module):\n",
    "    \"\"\"Complete HufuNet autoencoder.\"\"\"\n",
    "    def __init__(self, in_channels=3, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = HufuEncoder(in_channels, latent_dim)\n",
    "        self.decoder = HufuDecoder(in_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "# --- Classe Principale HufuNet DDPM ---\n",
    "\n",
    "class HufuDDPM:\n",
    "    def __init__(self, model_id, device=\"cuda\", secret_key=\"2020\"):\n",
    "        self.device = device\n",
    "        self.model_id = model_id\n",
    "        self.secret_key = secret_key\n",
    "\n",
    "        # Chargement du modele\n",
    "        self.pipeline = DDPMPipeline.from_pretrained(model_id)\n",
    "        self.unet = self.pipeline.unet.to(device)\n",
    "        self.scheduler = self.pipeline.scheduler\n",
    "\n",
    "        # Configuration par defaut\n",
    "        self.config = {\n",
    "            \"in_channels\": 3,\n",
    "            \"latent_dim\": 64,\n",
    "            \"lr\": 1e-4,\n",
    "            \"lr_ae\": 1e-3,         # Learning rate for autoencoder pretraining\n",
    "            \"ae_epochs\": 1,        # Epochs to pretrain autoencoder\n",
    "            \"epochs\": 100,           # Epochs for finetuning with watermark\n",
    "            \"mse_threshold\": 0.01,  # Threshold for ownership verification\n",
    "        }\n",
    "\n",
    "        self.saved_keys = {}\n",
    "\n",
    "    def _get_conv_params(self, model):\n",
    "        \"\"\"\n",
    "        Extract all conv weight parameters from the model.\n",
    "        Returns a flat parameter vector and layer info for reconstruction.\n",
    "        \"\"\"\n",
    "        all_params = []\n",
    "        layers_info = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'conv' in name.lower() and 'weight' in name:\n",
    "                all_params.append(param.view(-1))\n",
    "                layers_info.append({\n",
    "                    'name': name,\n",
    "                    'shape': param.shape,\n",
    "                    'numel': param.numel()\n",
    "                })\n",
    "\n",
    "        if len(all_params) == 0:\n",
    "            # Fallback: use all parameters\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    all_params.append(param.view(-1))\n",
    "                    layers_info.append({\n",
    "                        'name': name,\n",
    "                        'shape': param.shape,\n",
    "                        'numel': param.numel()\n",
    "                    })\n",
    "\n",
    "        param_vector = torch.cat(all_params)\n",
    "        return param_vector, layers_info\n",
    "\n",
    "    def _hash_position(self, decoder_value, index, total_params):\n",
    "        \"\"\"\n",
    "        Compute hash-based position for embedding using HMAC-SHA256.\n",
    "        \"\"\"\n",
    "        message = str(int(decoder_value * 1000) ^ index).encode()\n",
    "        mac = hmac.new(self.secret_key.encode(), message, hashlib.sha256)\n",
    "        position = int(mac.hexdigest(), 16) % total_params\n",
    "        return position\n",
    "\n",
    "    def _train_autoencoder(self, autoencoder, dataloader, epochs=5, freeze_decoder=False):\n",
    "        \"\"\"\n",
    "        Pre-train the HufuNet autoencoder.\n",
    "        \"\"\"\n",
    "        print(\"--- Pre-training HufuNet Autoencoder ---\")\n",
    "\n",
    "         # Freeze or unfreeze decoder\n",
    "        for param in autoencoder.decoder.parameters():\n",
    "            param.requires_grad = not freeze_decoder\n",
    "\n",
    "        if freeze_decoder:\n",
    "            print(\"Decoder is frozen — training encoder only.\")\n",
    "            optimizer = torch.optim.Adam(autoencoder.encoder.parameters(), lr=self.config[\"lr_ae\"])\n",
    "        else:\n",
    "            print(\"Decoder is trainable — training full autoencoder.\")\n",
    "            optimizer = torch.optim.Adam(autoencoder.parameters(), lr=self.config[\"lr_ae\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            pbar = tqdm(dataloader, desc=f\"AE Epoch {epoch+1}/{epochs}\")\n",
    "            total_loss = 0\n",
    "            for images, _ in pbar:\n",
    "                images = images.to(self.device)\n",
    "                # Normalize to [0, 1] for autoencoder\n",
    "                images_norm = (images + 1) / 2\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                _, reconstructed = autoencoder(images_norm)\n",
    "                loss = criterion(reconstructed, images_norm)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix(Loss=loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Avg Loss: {total_loss/len(dataloader):.4f}\")\n",
    "        for param in autoencoder.decoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        return autoencoder\n",
    "    def get_encoder_parameters(self, encoder):\n",
    "        \"\"\"\n",
    "        Extract encoder parameters into a flat vector for embedding.\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_params = []\n",
    "        for name, param in encoder.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                encoder_params.append(param.view(-1).detach())\n",
    "        encoder_vector = torch.cat(encoder_params)\n",
    "        return encoder_vector\n",
    "\n",
    "    def get_decoder_parameters(self, decoder):\n",
    "        \"\"\"\n",
    "        Extract decoder parameters into a flat vector for hashing.\n",
    "        \"\"\"\n",
    "        decoder_params = []\n",
    "        for name, param in decoder.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                decoder_params.append(param.view(-1).detach())\n",
    "        decoder_vector = torch.cat(decoder_params)\n",
    "        return decoder_vector\n",
    "\n",
    "    def embedded_positons_in_model(self, model, encoder_vector, decoder_vector):\n",
    "        \"\"\"\n",
    "        Embeds encoder parameters into the model weights using hash-based positioning.\n",
    "        \"\"\"\n",
    "        param_vector, layer_info = self._get_conv_params(model)\n",
    "        total_params = param_vector.numel()\n",
    "        watermark_size = encoder_vector.numel()\n",
    "\n",
    "        bitmap = torch.zeros(total_params, dtype=torch.bool, device=self.device)\n",
    "        embedded_positions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(min(watermark_size, total_params // 10)), desc=\"Embedding\"):\n",
    "                decoder_val = decoder_vector[i % len(decoder_vector)].item()\n",
    "                position = self._hash_position(decoder_val, i, total_params)\n",
    "\n",
    "                original_pos = position\n",
    "                while bitmap[position]:\n",
    "                    position = (position + 1) % total_params\n",
    "                    if position == original_pos:\n",
    "                        break\n",
    "\n",
    "                embedded_positions.append(position)\n",
    "                bitmap[position] = True\n",
    "\n",
    "\n",
    "        return embedded_positions, bitmap, layer_info\n",
    "\n",
    "    def _embed_params_into_model(self, model, encoder_vector, embedded_positions):\n",
    "        \"\"\"\n",
    "        Embeds encoder parameters into model weights at the specified positions.\n",
    "        Returns the modified model.\n",
    "        \"\"\"\n",
    "        # Build direct mapping: global_param_index -> encoder_value\n",
    "        position_to_encoder_value = {\n",
    "            pos: encoder_vector[i]\n",
    "            for i, pos in enumerate(embedded_positions)\n",
    "            if i < len(encoder_vector)\n",
    "        }\n",
    "\n",
    "        # Get model layers info\n",
    "        _, layers_info = self._get_conv_params(model)\n",
    "\n",
    "        # Embed into model\n",
    "        param_idx = 0\n",
    "        with torch.no_grad():\n",
    "            for info in layers_info:\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name == info['name']:\n",
    "                        param_flat = param.view(-1)\n",
    "                        for j in range(info['numel']):\n",
    "                            global_idx = param_idx + j\n",
    "                            if global_idx in position_to_encoder_value:\n",
    "                                param_flat[j] = position_to_encoder_value[global_idx]\n",
    "                        break\n",
    "                param_idx += info['numel']\n",
    "\n",
    "        return model\n",
    "    def _extract_and_evaluate(self, model, embedded_positions, decoder, dataloader):\n",
    "        \"\"\"\n",
    "        Extracts parameters from model at given positions,\n",
    "        reconstructs the encoder and full autoencoder,\n",
    "        evaluates MSE on dataloader.\n",
    "        Returns reconstructed autoencoder and avg MSE.\n",
    "        \"\"\"\n",
    "        # Get current model parameters\n",
    "        param_vector, _ = self._get_conv_params(model)\n",
    "\n",
    "        # Extract values at embedded positions\n",
    "        extracted_params = torch.zeros(len(embedded_positions), device=self.device)\n",
    "        with torch.no_grad():\n",
    "            for i, pos in enumerate(embedded_positions):\n",
    "                if pos < len(param_vector):\n",
    "                    extracted_params[i] = param_vector[pos]\n",
    "\n",
    "        # Reconstruct encoder by loading extracted values into architecture\n",
    "        reconstructed_encoder = HufuEncoder(\n",
    "            in_channels=self.config[\"in_channels\"],\n",
    "            latent_dim=self.config[\"latent_dim\"]\n",
    "        ).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "        offset = 0\n",
    "        with torch.no_grad():\n",
    "            for name, param in reconstructed_encoder.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    numel = param.numel()\n",
    "                    chunk = extracted_params[offset:offset + numel]\n",
    "                    if len(chunk) == numel:\n",
    "                        param.copy_(chunk.view(param.shape))\n",
    "                    offset += numel\n",
    "\n",
    "        # Rebuild full autoencoder with reconstructed encoder + owner decoder\n",
    "        reconstructed_autoencoder = HufuAutoencoder(\n",
    "            in_channels=self.config[\"in_channels\"],\n",
    "            latent_dim=self.config[\"latent_dim\"]\n",
    "        ).to(self.device)\n",
    "        reconstructed_autoencoder.encoder = reconstructed_encoder\n",
    "        reconstructed_autoencoder.decoder = decoder\n",
    "\n",
    "        # Evaluate MSE on dataloader\n",
    "        reconstructed_autoencoder.eval()\n",
    "        criterion = nn.MSELoss()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, _ in dataloader:\n",
    "                images = images.to(self.device)\n",
    "                images_norm = (images + 1) / 2\n",
    "                _, reconstructed = reconstructed_autoencoder(images_norm)\n",
    "                loss = criterion(reconstructed, images_norm)\n",
    "                total_loss += loss.item()\n",
    "                n_batches += 1\n",
    "                if n_batches >= 10:\n",
    "                    break\n",
    "\n",
    "        avg_mse = total_loss / n_batches\n",
    "        print(f\"Reconstructed autoencoder MSE: {avg_mse:.4f}\")\n",
    "\n",
    "        return reconstructed_autoencoder, avg_mse\n",
    "    def embed(self, dataloader):\n",
    "        \"\"\"\n",
    "        Embeds HufuNet encoder into DDPM model weights.\n",
    "        \"\"\"\n",
    "        print(f\"--- Demarrage Embedding HufuNet ---\")\n",
    "        autoencoder = HufuAutoencoder(\n",
    "            in_channels=self.config[\"in_channels\"],\n",
    "            latent_dim=self.config[\"latent_dim\"]\n",
    "        ).to(self.device)\n",
    "\n",
    "\n",
    "        # 1. Train autoencoder\n",
    "        autoencoder = self._train_autoencoder(autoencoder, dataloader, self.config[\"ae_epochs\"])\n",
    "        encoder = autoencoder.encoder\n",
    "        decoder = autoencoder.decoder\n",
    "\n",
    "\n",
    "        encoder_vector = self.get_encoder_parameters(encoder)\n",
    "        decoder_vector = self.get_decoder_parameters(decoder)\n",
    "        watermark_size = encoder_vector.numel()\n",
    "\n",
    "        # 4. Get model parameters\n",
    "        watermarked_unet = self.unet\n",
    "        embedded_positions, bitmap, layers_info=self.embedded_positons_in_model( watermarked_unet, encoder_vector, decoder_vector)\n",
    "\n",
    "\n",
    "        # 8. Fine-tune to maintain model quality\n",
    "        print(\"\\n--- Fine-tuning watermarked model ---\")\n",
    "        watermarked_unet.train()\n",
    "        optimizer = torch.optim.AdamW(watermarked_unet.parameters(), lr=self.config[\"lr\"])\n",
    "        mse_loss = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(self.config[\"epochs\"]):\n",
    "            encoder_vector = self.get_encoder_parameters(encoder)\n",
    "\n",
    "            watermarked_unet=self._embed_params_into_model(watermarked_unet, encoder_vector, embedded_positions)\n",
    "            pbar = tqdm(dataloader, desc=f\"Finetune Epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "\n",
    "            for clean_images, _ in pbar:\n",
    "                clean_images = clean_images.to(self.device)\n",
    "                bs = clean_images.shape[0]\n",
    "\n",
    "                noise = torch.randn_like(clean_images).to(self.device)\n",
    "                timesteps = torch.randint(0, self.scheduler.config.num_train_timesteps, (bs,), device=self.device).long()\n",
    "                noisy_images = self.scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                noise_pred = watermarked_unet(noisy_images, timesteps).sample\n",
    "                loss = mse_loss(noise_pred, noise)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.set_postfix(Loss=loss.item())\n",
    "            reconstructed_autoencoder, avg_mse=self._extract_and_evaluate(watermarked_unet, embedded_positions, decoder, dataloader)\n",
    "            print(f\"Epoch {epoch+1} - Finetune Loss: {loss.item():.4f} | Extracted Autoencoder MSE: {avg_mse:.4f}\")\n",
    "            if avg_mse <  self.config[\"mse_threshold\"]:\n",
    "                print(f\"MSE= {avg_mse} is acceptable, stopping fine-tuning to preserve watermark integrity.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"MSE= {avg_mse} is too high, continuing fine-tuning to improve autoencoder reconstruction.\")\n",
    "                autoencoder = self._train_autoencoder(reconstructed_autoencoder, dataloader, self.config[\"ae_epochs\"], freeze_decoder=True)\n",
    "                encoder = autoencoder.encoder\n",
    "\n",
    "        # Save keys\n",
    "        self.saved_keys = {\n",
    "            \"autoencoder\": reconstructed_autoencoder,\n",
    "            \"encoder\": reconstructed_autoencoder.encoder,\n",
    "            \"decoder\": reconstructed_autoencoder.decoder,\n",
    "            # \"encoder_vector\": encoder_vector,\n",
    "            # \"decoder_vector\": decoder_vector,\n",
    "            \"embedded_positions\": embedded_positions,\n",
    "            \"layers_info\": layers_info,\n",
    "            \"watermarked_unet\": watermarked_unet,\n",
    "        }\n",
    "        return watermarked_unet, reconstructed_autoencoder\n",
    "\n",
    "    # def extract(self, suspect_unet=None):\n",
    "    #     \"\"\"\n",
    "    #     Extracts the embedded encoder from a suspect model.\n",
    "    #     \"\"\"\n",
    "    #     if suspect_unet is None:\n",
    "    #         suspect_unet = self.saved_keys[\"watermarked_unet\"]\n",
    "    #\n",
    "    #     decoder_vector = self.saved_keys[\"decoder_vector\"]\n",
    "    #     embedded_positions = self.saved_keys[\"embedded_positions\"]\n",
    "    #     encoder_vector = self.saved_keys[\"encoder_vector\"]\n",
    "    #\n",
    "    #     # Get model parameters\n",
    "    #     param_vector, _ = self._get_conv_params(suspect_unet)\n",
    "    #\n",
    "    #     # Extract encoder parameters\n",
    "    #     extracted_encoder = torch.zeros_like(encoder_vector[:len(embedded_positions)])\n",
    "    #\n",
    "    #     with torch.no_grad():\n",
    "    #         for i, pos in enumerate(embedded_positions):\n",
    "    #             if pos < len(param_vector) and i < len(extracted_encoder):\n",
    "    #                 extracted_encoder[i] = param_vector[pos]\n",
    "    #\n",
    "    #     # Compute correlation/similarity with original encoder\n",
    "    #     original_encoder = encoder_vector[:len(embedded_positions)].to(self.device)\n",
    "    #     extracted_encoder = extracted_encoder.to(self.device)\n",
    "    #\n",
    "    #     mse = F.mse_loss(extracted_encoder, original_encoder).item()\n",
    "    #     correlation = F.cosine_similarity(\n",
    "    #         extracted_encoder.unsqueeze(0),\n",
    "    #         original_encoder.unsqueeze(0)\n",
    "    #     ).item()\n",
    "    #\n",
    "    #     print(f\"Extraction MSE: {mse:.4f}\")\n",
    "    #     print(f\"Correlation with original: {correlation:.4f}\")\n",
    "    #\n",
    "    #     return mse, correlation, extracted_encoder\n",
    "    def extract(self, model=None, dataloader=None):\n",
    "        \"\"\"\n",
    "        Extracts the embedded encoder from a suspect model and tests its functionality\n",
    "        by reconstructing images using the owner's decoder.\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            model = self.saved_keys[\"watermarked_unet\"]\n",
    "\n",
    "        decoder = self.saved_keys[\"decoder\"]\n",
    "        embedded_positions = self.saved_keys[\"embedded_positions\"]\n",
    "\n",
    "\n",
    "\n",
    "        reconstructed_autoencoder, avg_mse=self._extract_and_evaluate(model, embedded_positions, decoder, dataloader)\n",
    "        extracted_encoder = reconstructed_autoencoder.encoder\n",
    "\n",
    "\n",
    "        return avg_mse, extracted_encoder\n",
    "\n",
    "    # def verify(self, suspect_unet=None, test_dataloader=None):\n",
    "    #     \"\"\"\n",
    "    #     Verifies ownership by testing if extracted encoder + owner's decoder\n",
    "    #     can reconstruct images.\n",
    "    #     \"\"\"\n",
    "    #     if suspect_unet is None:\n",
    "    #         suspect_unet = self.saved_keys[\"watermarked_unet\"]\n",
    "    #\n",
    "    #     decoder = self.saved_keys[\"decoder\"]\n",
    "    #     autoencoder = self.saved_keys[\"autoencoder\"]\n",
    "    #\n",
    "    #     # Get extraction results\n",
    "    #     mse, correlation, extracted_encoder = self.extract(suspect_unet)\n",
    "    #\n",
    "    #     # Reconstruct using owner's autoencoder for comparison\n",
    "    #     if test_dataloader is not None:\n",
    "    #         autoencoder.eval()\n",
    "    #         criterion = nn.MSELoss()\n",
    "    #         total_loss = 0\n",
    "    #         n_batches = 0\n",
    "    #\n",
    "    #         with torch.no_grad():\n",
    "    #             for images, _ in test_dataloader:\n",
    "    #                 images = images.to(self.device)\n",
    "    #                 images_norm = (images + 1) / 2\n",
    "    #                 _, reconstructed = autoencoder(images_norm)\n",
    "    #                 loss = criterion(reconstructed, images_norm)\n",
    "    #                 total_loss += loss.item()\n",
    "    #                 n_batches += 1\n",
    "    #                 if n_batches >= 10:  # Limit batches for speed\n",
    "    #                     break\n",
    "    #\n",
    "    #         avg_loss = total_loss / n_batches\n",
    "    #         print(f\"Autoencoder reconstruction MSE: {avg_loss:.4f}\")\n",
    "    #\n",
    "    #     # Verification based on correlation\n",
    "    #     is_owner = correlation > 0.5\n",
    "    #     print(f\"\\nOwnership verified: {is_owner}\")\n",
    "    #\n",
    "    #     return is_owner, correlation\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/latim/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "embedding_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T15:35:45.754084228Z",
     "start_time": "2026-02-18T13:47:24.391125513Z"
    }
   },
   "source": [
    "# --- EXEMPLE D'EXECUTION ---\n",
    "\n",
    "# 1. Setup Data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 2. Embedding HufuNet\n",
    "hufu_defense = HufuDDPM(\"google/ddpm-cifar10-32\")\n",
    "watermarked_model = hufu_defense.embed(dataloader)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/latim/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80: Error no file named diffusion_pytorch_model.safetensors found in directory /home/latim/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 49.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Demarrage Embedding HufuNet ---\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is trainable — training full autoencoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 238.08it/s, Loss=0.00619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 56160/56160 [00:01<00:00, 51882.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuning watermarked model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 1: 100%|██████████| 782/782 [01:00<00:00, 12.82it/s, Loss=0.0179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1046\n",
      "Epoch 1 - Finetune Loss: 0.0179 | Extracted Autoencoder MSE: 0.1046\n",
      "MSE= 0.10457784980535507 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 254.91it/s, Loss=0.00526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 2: 100%|██████████| 782/782 [01:00<00:00, 12.94it/s, Loss=0.0126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1097\n",
      "Epoch 2 - Finetune Loss: 0.0126 | Extracted Autoencoder MSE: 0.1097\n",
      "MSE= 0.10968538075685501 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 258.53it/s, Loss=0.00479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 3: 100%|██████████| 782/782 [01:00<00:00, 12.84it/s, Loss=0.064] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1071\n",
      "Epoch 3 - Finetune Loss: 0.0640 | Extracted Autoencoder MSE: 0.1071\n",
      "MSE= 0.1071295477449894 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 245.88it/s, Loss=0.00476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 4: 100%|██████████| 782/782 [01:00<00:00, 12.93it/s, Loss=0.0223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1321\n",
      "Epoch 4 - Finetune Loss: 0.0223 | Extracted Autoencoder MSE: 0.1321\n",
      "MSE= 0.13211265057325364 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 245.96it/s, Loss=0.00589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 5: 100%|██████████| 782/782 [01:00<00:00, 12.93it/s, Loss=0.016] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1013\n",
      "Epoch 5 - Finetune Loss: 0.0160 | Extracted Autoencoder MSE: 0.1013\n",
      "MSE= 0.1012579046189785 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 254.41it/s, Loss=0.00543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 6: 100%|██████████| 782/782 [01:00<00:00, 12.94it/s, Loss=0.015] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1382\n",
      "Epoch 6 - Finetune Loss: 0.0150 | Extracted Autoencoder MSE: 0.1382\n",
      "MSE= 0.13822530806064606 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 246.40it/s, Loss=0.00516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 7: 100%|██████████| 782/782 [01:00<00:00, 12.93it/s, Loss=0.0877] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1023\n",
      "Epoch 7 - Finetune Loss: 0.0877 | Extracted Autoencoder MSE: 0.1023\n",
      "MSE= 0.10225981771945954 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:02<00:00, 262.44it/s, Loss=0.00398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 8: 100%|██████████| 782/782 [01:00<00:00, 12.85it/s, Loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0698\n",
      "Epoch 8 - Finetune Loss: 0.0123 | Extracted Autoencoder MSE: 0.0698\n",
      "MSE= 0.0697929546236992 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 228.80it/s, Loss=0.00528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 9: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1384\n",
      "Epoch 9 - Finetune Loss: 0.0311 | Extracted Autoencoder MSE: 0.1384\n",
      "MSE= 0.13840488344430923 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 244.17it/s, Loss=0.0052] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 10: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0457\n",
      "Epoch 10 - Finetune Loss: 0.0207 | Extracted Autoencoder MSE: 0.0457\n",
      "MSE= 0.045652090758085254 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 235.14it/s, Loss=0.00554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 11: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1879\n",
      "Epoch 11 - Finetune Loss: 0.0161 | Extracted Autoencoder MSE: 0.1879\n",
      "MSE= 0.18792126327753067 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 248.62it/s, Loss=0.00389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 12: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1213\n",
      "Epoch 12 - Finetune Loss: 0.0159 | Extracted Autoencoder MSE: 0.1213\n",
      "MSE= 0.12125628739595413 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 243.48it/s, Loss=0.00542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 13: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.2326\n",
      "Epoch 13 - Finetune Loss: 0.0627 | Extracted Autoencoder MSE: 0.2326\n",
      "MSE= 0.23261672854423524 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 249.39it/s, Loss=0.00438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 14: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0795] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0190\n",
      "Epoch 14 - Finetune Loss: 0.0795 | Extracted Autoencoder MSE: 0.0190\n",
      "MSE= 0.019023851677775382 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 245.92it/s, Loss=0.00529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 15: 100%|██████████| 782/782 [01:01<00:00, 12.78it/s, Loss=0.0102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1869\n",
      "Epoch 15 - Finetune Loss: 0.0102 | Extracted Autoencoder MSE: 0.1869\n",
      "MSE= 0.1869444727897644 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 243.67it/s, Loss=0.00482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 16: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0892\n",
      "Epoch 16 - Finetune Loss: 0.0296 | Extracted Autoencoder MSE: 0.0892\n",
      "MSE= 0.08922955244779587 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 233.55it/s, Loss=0.00518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 17: 100%|██████████| 782/782 [01:00<00:00, 12.84it/s, Loss=0.0205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.2062\n",
      "Epoch 17 - Finetune Loss: 0.0205 | Extracted Autoencoder MSE: 0.2062\n",
      "MSE= 0.20622168630361556 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 244.27it/s, Loss=0.00389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 18: 100%|██████████| 782/782 [01:01<00:00, 12.79it/s, Loss=0.0157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1029\n",
      "Epoch 18 - Finetune Loss: 0.0157 | Extracted Autoencoder MSE: 0.1029\n",
      "MSE= 0.10286379307508468 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 238.43it/s, Loss=0.00486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 19: 100%|██████████| 782/782 [01:01<00:00, 12.73it/s, Loss=0.0172] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0545\n",
      "Epoch 19 - Finetune Loss: 0.0172 | Extracted Autoencoder MSE: 0.0545\n",
      "MSE= 0.054488630220294 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 243.66it/s, Loss=0.00474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 20: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0270\n",
      "Epoch 20 - Finetune Loss: 0.0378 | Extracted Autoencoder MSE: 0.0270\n",
      "MSE= 0.026953640207648278 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 241.44it/s, Loss=0.00534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 21: 100%|██████████| 782/782 [01:00<00:00, 12.82it/s, Loss=0.021] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1371\n",
      "Epoch 21 - Finetune Loss: 0.0210 | Extracted Autoencoder MSE: 0.1371\n",
      "MSE= 0.13714545369148254 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 247.12it/s, Loss=0.00408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 22: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0339\n",
      "Epoch 22 - Finetune Loss: 0.0183 | Extracted Autoencoder MSE: 0.0339\n",
      "MSE= 0.033947964757680894 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 252.69it/s, Loss=0.00442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 23: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0638\n",
      "Epoch 23 - Finetune Loss: 0.0529 | Extracted Autoencoder MSE: 0.0638\n",
      "MSE= 0.06381191983819008 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 237.65it/s, Loss=0.00466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 24: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0492\n",
      "Epoch 24 - Finetune Loss: 0.0579 | Extracted Autoencoder MSE: 0.0492\n",
      "MSE= 0.04919848702847958 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 244.25it/s, Loss=0.00511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 25: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.2187\n",
      "Epoch 25 - Finetune Loss: 0.0255 | Extracted Autoencoder MSE: 0.2187\n",
      "MSE= 0.2187139719724655 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 257.19it/s, Loss=0.00366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 26: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0601\n",
      "Epoch 26 - Finetune Loss: 0.0452 | Extracted Autoencoder MSE: 0.0601\n",
      "MSE= 0.060114612057805064 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 230.42it/s, Loss=0.00383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 27: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1598\n",
      "Epoch 27 - Finetune Loss: 0.0801 | Extracted Autoencoder MSE: 0.1598\n",
      "MSE= 0.15980836302042006 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 247.38it/s, Loss=0.00542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 28: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1187\n",
      "Epoch 28 - Finetune Loss: 0.0309 | Extracted Autoencoder MSE: 0.1187\n",
      "MSE= 0.11873064488172531 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 241.57it/s, Loss=0.00522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 29: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.045] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0753\n",
      "Epoch 29 - Finetune Loss: 0.0450 | Extracted Autoencoder MSE: 0.0753\n",
      "MSE= 0.07528524622321128 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 240.28it/s, Loss=0.0033] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 30: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0278\n",
      "Epoch 30 - Finetune Loss: 0.0132 | Extracted Autoencoder MSE: 0.0278\n",
      "MSE= 0.027755442447960375 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 245.87it/s, Loss=0.00474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 31: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.00678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.2625\n",
      "Epoch 31 - Finetune Loss: 0.0068 | Extracted Autoencoder MSE: 0.2625\n",
      "MSE= 0.2624744713306427 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 244.77it/s, Loss=0.00464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 32: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0394] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1589\n",
      "Epoch 32 - Finetune Loss: 0.0394 | Extracted Autoencoder MSE: 0.1589\n",
      "MSE= 0.15888032764196397 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 252.93it/s, Loss=0.00483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 33: 100%|██████████| 782/782 [01:00<00:00, 12.82it/s, Loss=0.017]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0943\n",
      "Epoch 33 - Finetune Loss: 0.0170 | Extracted Autoencoder MSE: 0.0943\n",
      "MSE= 0.09425834938883781 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 253.81it/s, Loss=0.00375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 34: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0493\n",
      "Epoch 34 - Finetune Loss: 0.0207 | Extracted Autoencoder MSE: 0.0493\n",
      "MSE= 0.04932853989303112 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 223.03it/s, Loss=0.00485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 35: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0396] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0415\n",
      "Epoch 35 - Finetune Loss: 0.0396 | Extracted Autoencoder MSE: 0.0415\n",
      "MSE= 0.041505403071641925 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 248.40it/s, Loss=0.00589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 36: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0809\n",
      "Epoch 36 - Finetune Loss: 0.0133 | Extracted Autoencoder MSE: 0.0809\n",
      "MSE= 0.08088095858693123 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 251.30it/s, Loss=0.00352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 37: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0175\n",
      "Epoch 37 - Finetune Loss: 0.0122 | Extracted Autoencoder MSE: 0.0175\n",
      "MSE= 0.01749270409345627 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 237.43it/s, Loss=0.00412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 38: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0565\n",
      "Epoch 38 - Finetune Loss: 0.0214 | Extracted Autoencoder MSE: 0.0565\n",
      "MSE= 0.05653405040502548 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 251.34it/s, Loss=0.00467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 39: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0508\n",
      "Epoch 39 - Finetune Loss: 0.0102 | Extracted Autoencoder MSE: 0.0508\n",
      "MSE= 0.05084323510527611 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 243.83it/s, Loss=0.0036] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 40: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1050\n",
      "Epoch 40 - Finetune Loss: 0.0252 | Extracted Autoencoder MSE: 0.1050\n",
      "MSE= 0.10499133169651031 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 233.76it/s, Loss=0.00466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 41: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.016]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0168\n",
      "Epoch 41 - Finetune Loss: 0.0160 | Extracted Autoencoder MSE: 0.0168\n",
      "MSE= 0.016803418658673762 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 235.67it/s, Loss=0.00397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 42: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1950\n",
      "Epoch 42 - Finetune Loss: 0.0223 | Extracted Autoencoder MSE: 0.1950\n",
      "MSE= 0.1949920579791069 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 243.90it/s, Loss=0.00425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 43: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0206\n",
      "Epoch 43 - Finetune Loss: 0.0185 | Extracted Autoencoder MSE: 0.0206\n",
      "MSE= 0.020634367316961288 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 242.20it/s, Loss=0.00377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 44: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1871\n",
      "Epoch 44 - Finetune Loss: 0.0157 | Extracted Autoencoder MSE: 0.1871\n",
      "MSE= 0.18710490614175795 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 252.92it/s, Loss=0.00475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 45: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.00576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1731\n",
      "Epoch 45 - Finetune Loss: 0.0058 | Extracted Autoencoder MSE: 0.1731\n",
      "MSE= 0.17307330816984176 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 244.13it/s, Loss=0.00433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 46: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0962\n",
      "Epoch 46 - Finetune Loss: 0.0268 | Extracted Autoencoder MSE: 0.0962\n",
      "MSE= 0.09620324224233627 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 217.72it/s, Loss=0.00469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 47: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0996\n",
      "Epoch 47 - Finetune Loss: 0.0258 | Extracted Autoencoder MSE: 0.0996\n",
      "MSE= 0.09960262104868889 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 237.62it/s, Loss=0.00474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 48: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0741\n",
      "Epoch 48 - Finetune Loss: 0.0152 | Extracted Autoencoder MSE: 0.0741\n",
      "MSE= 0.0740594021975994 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 213.78it/s, Loss=0.00469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 49: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1582\n",
      "Epoch 49 - Finetune Loss: 0.0144 | Extracted Autoencoder MSE: 0.1582\n",
      "MSE= 0.1582486629486084 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 244.89it/s, Loss=0.00431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 50: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0224] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0373\n",
      "Epoch 50 - Finetune Loss: 0.0224 | Extracted Autoencoder MSE: 0.0373\n",
      "MSE= 0.037328314408659934 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 245.12it/s, Loss=0.00363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 51: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0217] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0765\n",
      "Epoch 51 - Finetune Loss: 0.0217 | Extracted Autoencoder MSE: 0.0765\n",
      "MSE= 0.07653845250606536 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 248.73it/s, Loss=0.00286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 52: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0914\n",
      "Epoch 52 - Finetune Loss: 0.0335 | Extracted Autoencoder MSE: 0.0914\n",
      "MSE= 0.09139428958296776 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 245.95it/s, Loss=0.00505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 53: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0128\n",
      "Epoch 53 - Finetune Loss: 0.0343 | Extracted Autoencoder MSE: 0.0128\n",
      "MSE= 0.012762391939759254 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 244.49it/s, Loss=0.00417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 54: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1847\n",
      "Epoch 54 - Finetune Loss: 0.0148 | Extracted Autoencoder MSE: 0.1847\n",
      "MSE= 0.18472839146852493 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 248.07it/s, Loss=0.00394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 55: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.05]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0799\n",
      "Epoch 55 - Finetune Loss: 0.0500 | Extracted Autoencoder MSE: 0.0799\n",
      "MSE= 0.0798702098429203 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 218.01it/s, Loss=0.00417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 56: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1211\n",
      "Epoch 56 - Finetune Loss: 0.0306 | Extracted Autoencoder MSE: 0.1211\n",
      "MSE= 0.12111465781927108 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 244.69it/s, Loss=0.00356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 57: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0196] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1418\n",
      "Epoch 57 - Finetune Loss: 0.0196 | Extracted Autoencoder MSE: 0.1418\n",
      "MSE= 0.1418408617377281 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 252.22it/s, Loss=0.00368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 58: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0922\n",
      "Epoch 58 - Finetune Loss: 0.0123 | Extracted Autoencoder MSE: 0.0922\n",
      "MSE= 0.09220858216285706 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 240.06it/s, Loss=0.00414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 59: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0373\n",
      "Epoch 59 - Finetune Loss: 0.0159 | Extracted Autoencoder MSE: 0.0373\n",
      "MSE= 0.03728825151920319 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 246.60it/s, Loss=0.0053] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 60: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0388\n",
      "Epoch 60 - Finetune Loss: 0.0159 | Extracted Autoencoder MSE: 0.0388\n",
      "MSE= 0.03883764371275902 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 252.23it/s, Loss=0.00573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 61: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1502\n",
      "Epoch 61 - Finetune Loss: 0.0229 | Extracted Autoencoder MSE: 0.1502\n",
      "MSE= 0.15017532110214232 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 249.57it/s, Loss=0.00388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 62: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0354] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0960\n",
      "Epoch 62 - Finetune Loss: 0.0354 | Extracted Autoencoder MSE: 0.0960\n",
      "MSE= 0.09599229916930199 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 246.07it/s, Loss=0.00433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 63: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1078\n",
      "Epoch 63 - Finetune Loss: 0.0335 | Extracted Autoencoder MSE: 0.1078\n",
      "MSE= 0.10775829777121544 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 249.01it/s, Loss=0.00463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 64: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.00992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.2056\n",
      "Epoch 64 - Finetune Loss: 0.0099 | Extracted Autoencoder MSE: 0.2056\n",
      "MSE= 0.2056385412812233 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 248.60it/s, Loss=0.00481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 65: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1669\n",
      "Epoch 65 - Finetune Loss: 0.0324 | Extracted Autoencoder MSE: 0.1669\n",
      "MSE= 0.1669479340314865 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 251.81it/s, Loss=0.00448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 66: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1043\n",
      "Epoch 66 - Finetune Loss: 0.0215 | Extracted Autoencoder MSE: 0.1043\n",
      "MSE= 0.10434033572673798 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 250.55it/s, Loss=0.00502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 67: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0534\n",
      "Epoch 67 - Finetune Loss: 0.0319 | Extracted Autoencoder MSE: 0.0534\n",
      "MSE= 0.0534202229231596 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 240.58it/s, Loss=0.00379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 68: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1206\n",
      "Epoch 68 - Finetune Loss: 0.0134 | Extracted Autoencoder MSE: 0.1206\n",
      "MSE= 0.12055368721485138 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 252.45it/s, Loss=0.00448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 69: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.011]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1691\n",
      "Epoch 69 - Finetune Loss: 0.0110 | Extracted Autoencoder MSE: 0.1691\n",
      "MSE= 0.16910132467746736 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 231.82it/s, Loss=0.00478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 70: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0318] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0821\n",
      "Epoch 70 - Finetune Loss: 0.0318 | Extracted Autoencoder MSE: 0.0821\n",
      "MSE= 0.08205361813306808 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 229.91it/s, Loss=0.00477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 71: 100%|██████████| 782/782 [01:00<00:00, 12.82it/s, Loss=0.00864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0473\n",
      "Epoch 71 - Finetune Loss: 0.0086 | Extracted Autoencoder MSE: 0.0473\n",
      "MSE= 0.04727691188454628 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 228.42it/s, Loss=0.00536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 72: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0949\n",
      "Epoch 72 - Finetune Loss: 0.0503 | Extracted Autoencoder MSE: 0.0949\n",
      "MSE= 0.09492490068078041 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 212.68it/s, Loss=0.00386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 73: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0232] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0182\n",
      "Epoch 73 - Finetune Loss: 0.0232 | Extracted Autoencoder MSE: 0.0182\n",
      "MSE= 0.018205696530640126 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 242.41it/s, Loss=0.00393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 74: 100%|██████████| 782/782 [01:01<00:00, 12.77it/s, Loss=0.0328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1328\n",
      "Epoch 74 - Finetune Loss: 0.0328 | Extracted Autoencoder MSE: 0.1328\n",
      "MSE= 0.13278324156999588 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 247.27it/s, Loss=0.00354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 75: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0416] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0288\n",
      "Epoch 75 - Finetune Loss: 0.0416 | Extracted Autoencoder MSE: 0.0288\n",
      "MSE= 0.02880984526127577 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 196.32it/s, Loss=0.00529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 76: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1220\n",
      "Epoch 76 - Finetune Loss: 0.0225 | Extracted Autoencoder MSE: 0.1220\n",
      "MSE= 0.12197931036353112 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 245.99it/s, Loss=0.00422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 77: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.015] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1911\n",
      "Epoch 77 - Finetune Loss: 0.0150 | Extracted Autoencoder MSE: 0.1911\n",
      "MSE= 0.19107250720262528 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 218.95it/s, Loss=0.00325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 78: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0100\n",
      "Epoch 78 - Finetune Loss: 0.0446 | Extracted Autoencoder MSE: 0.0100\n",
      "MSE= 0.010002575721591711 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 234.45it/s, Loss=0.0047] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 79: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s, Loss=0.0142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0653\n",
      "Epoch 79 - Finetune Loss: 0.0142 | Extracted Autoencoder MSE: 0.0653\n",
      "MSE= 0.06528294309973717 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 247.48it/s, Loss=0.00391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 80: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.013] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1722\n",
      "Epoch 80 - Finetune Loss: 0.0130 | Extracted Autoencoder MSE: 0.1722\n",
      "MSE= 0.17221349626779556 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 238.74it/s, Loss=0.00494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 81: 100%|██████████| 782/782 [01:00<00:00, 12.82it/s, Loss=0.0237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.2036\n",
      "Epoch 81 - Finetune Loss: 0.0237 | Extracted Autoencoder MSE: 0.2036\n",
      "MSE= 0.20363398343324662 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 212.08it/s, Loss=0.00437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 82: 100%|██████████| 782/782 [01:00<00:00, 12.82it/s, Loss=0.0251] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0988\n",
      "Epoch 82 - Finetune Loss: 0.0251 | Extracted Autoencoder MSE: 0.0988\n",
      "MSE= 0.0987573467195034 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 245.81it/s, Loss=0.00476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 83: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1504\n",
      "Epoch 83 - Finetune Loss: 0.0153 | Extracted Autoencoder MSE: 0.1504\n",
      "MSE= 0.15042689442634583 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 247.08it/s, Loss=0.00383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 84: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0277] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0961\n",
      "Epoch 84 - Finetune Loss: 0.0277 | Extracted Autoencoder MSE: 0.0961\n",
      "MSE= 0.09605187177658081 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 244.37it/s, Loss=0.00516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 85: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.0364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1059\n",
      "Epoch 85 - Finetune Loss: 0.0364 | Extracted Autoencoder MSE: 0.1059\n",
      "MSE= 0.10592763721942902 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 248.46it/s, Loss=0.00547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 86: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0349] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0406\n",
      "Epoch 86 - Finetune Loss: 0.0349 | Extracted Autoencoder MSE: 0.0406\n",
      "MSE= 0.040580964833498004 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 249.05it/s, Loss=0.00479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 87: 100%|██████████| 782/782 [01:00<00:00, 12.82it/s, Loss=0.00985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0171\n",
      "Epoch 87 - Finetune Loss: 0.0098 | Extracted Autoencoder MSE: 0.0171\n",
      "MSE= 0.01706421244889498 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 246.64it/s, Loss=0.00457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 88: 100%|██████████| 782/782 [01:01<00:00, 12.79it/s, Loss=0.0273] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0759\n",
      "Epoch 88 - Finetune Loss: 0.0273 | Extracted Autoencoder MSE: 0.0759\n",
      "MSE= 0.07589620277285576 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 251.54it/s, Loss=0.00424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 89: 100%|██████████| 782/782 [01:00<00:00, 12.82it/s, Loss=0.00816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0519\n",
      "Epoch 89 - Finetune Loss: 0.0082 | Extracted Autoencoder MSE: 0.0519\n",
      "MSE= 0.05186670869588852 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 242.26it/s, Loss=0.00441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 90: 100%|██████████| 782/782 [01:01<00:00, 12.77it/s, Loss=0.0301] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0115\n",
      "Epoch 90 - Finetune Loss: 0.0301 | Extracted Autoencoder MSE: 0.0115\n",
      "MSE= 0.011478450056165457 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 236.64it/s, Loss=0.00419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 91: 100%|██████████| 782/782 [01:00<00:00, 12.82it/s, Loss=0.0276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1433\n",
      "Epoch 91 - Finetune Loss: 0.0276 | Extracted Autoencoder MSE: 0.1433\n",
      "MSE= 0.1433245450258255 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 246.01it/s, Loss=0.00374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 92: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0669] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1094\n",
      "Epoch 92 - Finetune Loss: 0.0669 | Extracted Autoencoder MSE: 0.1094\n",
      "MSE= 0.10938173159956932 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 249.36it/s, Loss=0.00424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 93: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0240\n",
      "Epoch 93 - Finetune Loss: 0.0161 | Extracted Autoencoder MSE: 0.0240\n",
      "MSE= 0.023975613713264465 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 228.92it/s, Loss=0.00422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 94: 100%|██████████| 782/782 [01:01<00:00, 12.81it/s, Loss=0.0206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0332\n",
      "Epoch 94 - Finetune Loss: 0.0206 | Extracted Autoencoder MSE: 0.0332\n",
      "MSE= 0.03321850001811981 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 247.11it/s, Loss=0.00504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 95: 100%|██████████| 782/782 [01:01<00:00, 12.82it/s, Loss=0.00986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1826\n",
      "Epoch 95 - Finetune Loss: 0.0099 | Extracted Autoencoder MSE: 0.1826\n",
      "MSE= 0.18261546045541763 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 236.07it/s, Loss=0.00505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 96: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1976\n",
      "Epoch 96 - Finetune Loss: 0.0146 | Extracted Autoencoder MSE: 0.1976\n",
      "MSE= 0.19758164137601852 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 252.06it/s, Loss=0.00374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 97: 100%|██████████| 782/782 [01:01<00:00, 12.79it/s, Loss=0.0406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0224\n",
      "Epoch 97 - Finetune Loss: 0.0406 | Extracted Autoencoder MSE: 0.0224\n",
      "MSE= 0.022400523722171783 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 226.92it/s, Loss=0.00449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 98: 100%|██████████| 782/782 [01:01<00:00, 12.80it/s, Loss=0.0269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.2045\n",
      "Epoch 98 - Finetune Loss: 0.0269 | Extracted Autoencoder MSE: 0.2045\n",
      "MSE= 0.20451595336198808 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n",
      "Decoder is frozen — training encoder only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 782/782 [00:03<00:00, 234.61it/s, Loss=0.00427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 99: 100%|██████████| 782/782 [01:01<00:00, 12.77it/s, Loss=0.0147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0071\n",
      "Epoch 99 - Finetune Loss: 0.0147 | Extracted Autoencoder MSE: 0.0071\n",
      "MSE= 0.007083063432946801 is acceptable, stopping fine-tuning to preserve watermark integrity.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "distillation_attack_cell",
   "metadata": {},
   "source": [
    "# --- Fonction de Distillation (Attaque) ---\n",
    "\n",
    "def run_distillation_attack_hufu(hufu_obj, dataloader, test_dataloader, epochs=5, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Tente de transferer la fonctionnalite du modele HufuNet vers un modele vierge.\n",
    "    Verifie si le watermark survit.\n",
    "    \"\"\"\n",
    "    device = hufu_obj.device\n",
    "\n",
    "    # 1. Teacher (Gele)\n",
    "    teacher_unet = hufu_obj.saved_keys[\"watermarked_unet\"]\n",
    "    teacher_unet.eval()\n",
    "    for p in teacher_unet.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 2. Student (Vierge - Meme architecture)\n",
    "    print(\"\\n--- Initialisation du Student ---\")\n",
    "    student_pipeline = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "    student_unet = student_pipeline.unet.to(device)\n",
    "    student_unet.train()\n",
    "\n",
    "\n",
    "\n",
    "    mse_teacher, _ = hufu_obj.extract(teacher_unet, dataloader)\n",
    "    print(\"\\n[Check] Teacher: mse_hufu_teacher: {:.4f}\".format(mse_teacher))\n",
    "\n",
    "    mse_student, _ = hufu_obj.extract(student_unet, dataloader)\n",
    "    print(\"\\n[Check] Student (before): mse_hufu_student: {:.4f}\".format(mse_student))\n",
    "\n",
    "    optimizer = AdamW(student_unet.parameters(), lr=lr)\n",
    "    noise_scheduler = hufu_obj.scheduler\n",
    "    history = {\"loss\": [], \"MSE\": []}\n",
    "\n",
    "    print(f\"\\n--- Distillation HufuNet ({epochs} epochs) ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for clean_images, _ in pbar:\n",
    "            clean_images = clean_images.to(device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            noise = torch.randn_like(clean_images).to(device)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=device).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                target_pred = teacher_unet(noisy_images, timesteps).sample\n",
    "\n",
    "            student_pred = student_unet(noisy_images, timesteps).sample\n",
    "\n",
    "            loss = F.mse_loss(student_pred, target_pred)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix(Loss=loss.item())\n",
    "\n",
    "        # Check correlation\n",
    "        mse , _ = hufu_obj.extract(student_unet, dataloader)\n",
    "        history[\"MSE\"].append(mse)\n",
    "        history[\"loss\"].append(running_loss / len(dataloader))\n",
    "\n",
    "        print(f\"Fin Epoch {epoch+1} | Loss: {history['loss'][-1]:.4f} | mse_student: {mse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    return student_unet, history\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "run_attack_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:22:03.628403659Z",
     "start_time": "2026-02-18T15:35:45.857372998Z"
    }
   },
   "source": [
    "# 5. Attaque par Distillation\n",
    "student_res, stats = run_distillation_attack_hufu(hufu_defense, dataloader, test_dataloader, epochs=100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initialisation du Student ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/latim/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80: Error no file named diffusion_pytorch_model.safetensors found in directory /home/latim/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0440\n",
      "\n",
      "[Check] Teacher: mse_hufu_teacher: 0.0440\n",
      "Reconstructed autoencoder MSE: 0.0660\n",
      "\n",
      "[Check] Student (before): mse_hufu_student: 0.0660\n",
      "\n",
      "--- Distillation HufuNet (100 epochs) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 782/782 [01:17<00:00, 10.15it/s, Loss=0.000948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0696\n",
      "Fin Epoch 1 | Loss: 0.0007 | mse_student: 0.0696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 782/782 [01:17<00:00, 10.15it/s, Loss=0.000114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0640\n",
      "Fin Epoch 2 | Loss: 0.0006 | mse_student: 0.0640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 782/782 [01:17<00:00, 10.15it/s, Loss=0.000691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0667\n",
      "Fin Epoch 3 | Loss: 0.0005 | mse_student: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 782/782 [01:17<00:00, 10.16it/s, Loss=0.000517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0677\n",
      "Fin Epoch 4 | Loss: 0.0005 | mse_student: 0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 782/782 [01:16<00:00, 10.16it/s, Loss=0.000368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0685\n",
      "Fin Epoch 5 | Loss: 0.0004 | mse_student: 0.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 782/782 [01:17<00:00, 10.08it/s, Loss=0.000319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0696\n",
      "Fin Epoch 6 | Loss: 0.0004 | mse_student: 0.0696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 782/782 [01:16<00:00, 10.19it/s, Loss=0.000312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0707\n",
      "Fin Epoch 7 | Loss: 0.0004 | mse_student: 0.0707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 782/782 [01:16<00:00, 10.17it/s, Loss=0.000323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0688\n",
      "Fin Epoch 8 | Loss: 0.0004 | mse_student: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 782/782 [01:16<00:00, 10.20it/s, Loss=0.000171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0686\n",
      "Fin Epoch 9 | Loss: 0.0003 | mse_student: 0.0686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 782/782 [01:17<00:00, 10.10it/s, Loss=0.000477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0705\n",
      "Fin Epoch 10 | Loss: 0.0003 | mse_student: 0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 782/782 [01:17<00:00, 10.13it/s, Loss=0.000433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0695\n",
      "Fin Epoch 11 | Loss: 0.0003 | mse_student: 0.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 782/782 [01:16<00:00, 10.23it/s, Loss=0.000159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0756\n",
      "Fin Epoch 12 | Loss: 0.0003 | mse_student: 0.0756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 782/782 [01:16<00:00, 10.23it/s, Loss=0.000278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0700\n",
      "Fin Epoch 13 | Loss: 0.0003 | mse_student: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 782/782 [01:16<00:00, 10.23it/s, Loss=0.000129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0683\n",
      "Fin Epoch 14 | Loss: 0.0003 | mse_student: 0.0683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 782/782 [01:17<00:00, 10.07it/s, Loss=0.000238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0718\n",
      "Fin Epoch 15 | Loss: 0.0003 | mse_student: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 782/782 [01:16<00:00, 10.18it/s, Loss=0.000287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0719\n",
      "Fin Epoch 16 | Loss: 0.0002 | mse_student: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 782/782 [01:16<00:00, 10.17it/s, Loss=0.000145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0744\n",
      "Fin Epoch 17 | Loss: 0.0002 | mse_student: 0.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 782/782 [01:16<00:00, 10.19it/s, Loss=0.000358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0709\n",
      "Fin Epoch 18 | Loss: 0.0002 | mse_student: 0.0709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 782/782 [01:16<00:00, 10.20it/s, Loss=0.00024] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0694\n",
      "Fin Epoch 19 | Loss: 0.0002 | mse_student: 0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 782/782 [01:16<00:00, 10.22it/s, Loss=0.000388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0720\n",
      "Fin Epoch 20 | Loss: 0.0002 | mse_student: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 782/782 [01:16<00:00, 10.23it/s, Loss=0.000152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0708\n",
      "Fin Epoch 21 | Loss: 0.0002 | mse_student: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 782/782 [01:16<00:00, 10.23it/s, Loss=0.000167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0751\n",
      "Fin Epoch 22 | Loss: 0.0002 | mse_student: 0.0751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 782/782 [01:17<00:00, 10.14it/s, Loss=0.000196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0783\n",
      "Fin Epoch 23 | Loss: 0.0002 | mse_student: 0.0783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 782/782 [01:16<00:00, 10.20it/s, Loss=0.000224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0749\n",
      "Fin Epoch 24 | Loss: 0.0002 | mse_student: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 782/782 [01:16<00:00, 10.18it/s, Loss=0.000193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0797\n",
      "Fin Epoch 25 | Loss: 0.0002 | mse_student: 0.0797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 782/782 [01:16<00:00, 10.17it/s, Loss=0.000242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0735\n",
      "Fin Epoch 26 | Loss: 0.0002 | mse_student: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 782/782 [01:16<00:00, 10.23it/s, Loss=0.000149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0791\n",
      "Fin Epoch 27 | Loss: 0.0002 | mse_student: 0.0791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 782/782 [01:16<00:00, 10.16it/s, Loss=0.000196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0742\n",
      "Fin Epoch 28 | Loss: 0.0002 | mse_student: 0.0742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 782/782 [01:16<00:00, 10.20it/s, Loss=0.00011] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0793\n",
      "Fin Epoch 29 | Loss: 0.0002 | mse_student: 0.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 782/782 [01:16<00:00, 10.20it/s, Loss=0.000208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0760\n",
      "Fin Epoch 30 | Loss: 0.0002 | mse_student: 0.0760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 782/782 [01:16<00:00, 10.16it/s, Loss=0.000127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0748\n",
      "Fin Epoch 31 | Loss: 0.0002 | mse_student: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 782/782 [01:16<00:00, 10.18it/s, Loss=5.1e-5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0783\n",
      "Fin Epoch 32 | Loss: 0.0002 | mse_student: 0.0783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 782/782 [01:16<00:00, 10.20it/s, Loss=0.000156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0838\n",
      "Fin Epoch 33 | Loss: 0.0002 | mse_student: 0.0838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 782/782 [01:16<00:00, 10.19it/s, Loss=0.000176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0747\n",
      "Fin Epoch 34 | Loss: 0.0002 | mse_student: 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 782/782 [01:16<00:00, 10.20it/s, Loss=0.00016] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0759\n",
      "Fin Epoch 35 | Loss: 0.0002 | mse_student: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 782/782 [01:16<00:00, 10.19it/s, Loss=0.000272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0800\n",
      "Fin Epoch 36 | Loss: 0.0002 | mse_student: 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37:   3%|▎         | 21/782 [00:02<01:18,  9.75it/s, Loss=0.000168]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# 5. Attaque par Distillation\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m student_res, stats = \u001B[43mrun_distillation_attack_hufu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhufu_defense\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 56\u001B[39m, in \u001B[36mrun_distillation_attack_hufu\u001B[39m\u001B[34m(hufu_obj, dataloader, test_dataloader, epochs, lr)\u001B[39m\n\u001B[32m     53\u001B[39m loss = F.mse_loss(student_pred, target_pred)\n\u001B[32m     55\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     57\u001B[39m optimizer.step()\n\u001B[32m     59\u001B[39m running_loss += loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "plot_results_cell",
   "metadata": {},
   "source": [
    "# 6. Visualisation des resultats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(stats[\"loss\"])\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Distillation Loss\")\n",
    "\n",
    "ax2.plot(stats[\"correlation\"])\n",
    "ax2.axhline(y=0.5, color='r', linestyle='--', label='Threshold (0.5)')\n",
    "ax2.axhline(y=0.0, color='g', linestyle='--', label='No correlation (0.0)')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Correlation\")\n",
    "ax2.set_title(\"Encoder Correlation During Distillation\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "visualize_ae_cell",
   "metadata": {},
   "source": [
    "# 7. Visualize Autoencoder Reconstruction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "autoencoder = hufu_defense.saved_keys[\"autoencoder\"]\n",
    "autoencoder.eval()\n",
    "\n",
    "# Get some test images\n",
    "test_images, _ = next(iter(test_dataloader))\n",
    "test_images = test_images[:8].to(hufu_defense.device)\n",
    "test_images_norm = (test_images + 1) / 2\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, reconstructed = autoencoder(test_images_norm)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    img = test_images_norm[i].cpu().permute(1, 2, 0).numpy()\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original')\n",
    "\n",
    "    # Reconstructed\n",
    "    rec = reconstructed[i].cpu().permute(1, 2, 0).numpy()\n",
    "    axes[1, i].imshow(rec)\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Reconstructed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

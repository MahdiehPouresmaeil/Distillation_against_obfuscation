{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T10:24:56.445004199Z",
     "start_time": "2026-02-18T10:24:56.395377949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import psutil\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=200):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder: 4 Conv2d layers with stride=2, kernel=4, padding=1\n",
    "        # Input: (3, 64, 64) -> (32, 32, 32) -> (64, 16, 16) -> (128, 8, 8) -> (256, 4, 4)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),    # 0\n",
    "            nn.ReLU(),                                                # 1\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),   # 2\n",
    "            nn.ReLU(),                                                # 3\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 4\n",
    "            nn.ReLU(),                                                # 5\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # 6\n",
    "            nn.ReLU(),                                                # 7\n",
    "        )\n",
    "\n",
    "        # 256 * 4 * 4 = 4096\n",
    "        self.fc_mu = nn.Linear(4096, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(4096, latent_dim)\n",
    "\n",
    "        # Decoder input\n",
    "        self.decoder_input = nn.Linear(latent_dim, 4096)\n",
    "\n",
    "        # Decoder: 4 ConvTranspose2d layers\n",
    "        # (256, 4, 4) -> (128, 8, 8) -> (64, 16, 16) -> (32, 32, 32) -> (3, 64, 64)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 0\n",
    "            nn.ReLU(),                                                          # 1\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 2\n",
    "            nn.ReLU(),                                                          # 3\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 4\n",
    "            nn.ReLU(),                                                          # 5\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),     # 6\n",
    "            nn.Sigmoid(),                                                       # 7\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, 4096)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(x.size(0), 256, 4, 4)  # Reshape to (batch, 256, 4, 4)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n"
   ],
   "id": "e2ef6b1811dcdf77",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T10:24:56.986125432Z",
     "start_time": "2026-02-18T10:24:56.943729005Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from torch.optim import AdamW\n",
    "import hmac\n",
    "import hashlib\n",
    "\n",
    "# --- HufuNet Autoencoder ---\n",
    "\n",
    "class HufuEncoder(nn.Module):\n",
    "    \"\"\"Encoder part of HufuNet autoencoder.\"\"\"\n",
    "    def __init__(self, in_channels=3, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, latent_dim, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class HufuDecoder(nn.Module):\n",
    "    \"\"\"Decoder part of HufuNet autoencoder.\"\"\"\n",
    "    def __init__(self, in_channels=3, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, in_channels, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class HufuAutoencoder(nn.Module):\n",
    "    \"\"\"Complete HufuNet autoencoder.\"\"\"\n",
    "    def __init__(self, in_channels=3, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = HufuEncoder(in_channels, latent_dim)\n",
    "        self.decoder = HufuDecoder(in_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "# --- Classe Principale HufuNet DDPM ---\n",
    "\n",
    "class HufuVAE:\n",
    "    def __init__(self, model, device=\"cuda\", secret_key=\"2020\"):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.secret_key = secret_key\n",
    "\n",
    "\n",
    "        # Configuration par defaut\n",
    "        self.config = {\n",
    "            \"in_channels\": 3,\n",
    "            \"latent_dim\":32,\n",
    "            \"lr\": 1e-4,\n",
    "            \"lr_ae\": 1e-3,         # Learning rate for autoencoder pretraining\n",
    "            \"ae_epochs\": 1,        # Epochs to pretrain autoencoder\n",
    "            \"epochs\": 100,           # Epochs for finetuning with watermark\n",
    "            \"beta_kl\":1.0,\n",
    "            \"mse_threshold\":0.01\n",
    "        }\n",
    "\n",
    "        self.saved_keys = {}\n",
    "\n",
    "    def _get_conv_params(self, model):\n",
    "        \"\"\"\n",
    "        Extract all conv weight parameters from the model.\n",
    "        Returns a flat parameter vector and layer info for reconstruction.\n",
    "        \"\"\"\n",
    "        all_params = []\n",
    "        layers_info = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'conv' in name.lower() and 'weight' in name:\n",
    "                all_params.append(param.view(-1))\n",
    "                layers_info.append({\n",
    "                    'name': name,\n",
    "                    'shape': param.shape,\n",
    "                    'numel': param.numel()\n",
    "                })\n",
    "\n",
    "        if len(all_params) == 0:\n",
    "            # Fallback: use all parameters\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    all_params.append(param.view(-1))\n",
    "                    layers_info.append({\n",
    "                        'name': name,\n",
    "                        'shape': param.shape,\n",
    "                        'numel': param.numel()\n",
    "                    })\n",
    "\n",
    "        param_vector = torch.cat(all_params)\n",
    "        return param_vector, layers_info\n",
    "\n",
    "    def _hash_position(self, decoder_value, index, total_params):\n",
    "        \"\"\"\n",
    "        Compute hash-based position for embedding using HMAC-SHA256.\n",
    "        \"\"\"\n",
    "        message = str(int(decoder_value * 1000) ^ index).encode()\n",
    "        mac = hmac.new(self.secret_key.encode(), message, hashlib.sha256)\n",
    "        position = int(mac.hexdigest(), 16) % total_params\n",
    "        return position\n",
    "\n",
    "    def _train_autoencoder(self, autoencoder, dataloader, epochs=5):\n",
    "        \"\"\"\n",
    "        Pre-train the HufuNet autoencoder.\n",
    "        \"\"\"\n",
    "        print(\"--- Pre-training HufuNet Autoencoder ---\")\n",
    "\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=self.config[\"lr_ae\"])\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            pbar = tqdm(dataloader, desc=f\"AE Epoch {epoch+1}/{epochs}\")\n",
    "            total_loss = 0\n",
    "            for images, _ in pbar:\n",
    "                images = images.to(self.device)\n",
    "                # Normalize to [0, 1] for autoencoder\n",
    "                images_norm = (images + 1) / 2\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                _, reconstructed = autoencoder(images_norm)\n",
    "                loss = criterion(reconstructed, images_norm)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix(Loss=loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Avg Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "        return autoencoder\n",
    "    def get_encoder_parameters(self, encoder):\n",
    "        \"\"\"\n",
    "        Extract encoder parameters into a flat vector for embedding.\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_params = []\n",
    "        for name, param in encoder.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                encoder_params.append(param.view(-1).detach())\n",
    "        encoder_vector = torch.cat(encoder_params)\n",
    "        return encoder_vector\n",
    "\n",
    "    def get_decoder_parameters(self, decoder):\n",
    "        \"\"\"\n",
    "        Extract decoder parameters into a flat vector for hashing.\n",
    "        \"\"\"\n",
    "        decoder_params = []\n",
    "        for name, param in decoder.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                decoder_params.append(param.view(-1).detach())\n",
    "        decoder_vector = torch.cat(decoder_params)\n",
    "        return decoder_vector\n",
    "\n",
    "    def embedded_positons_in_model(self, model, encoder_vector, decoder_vector):\n",
    "        \"\"\"\n",
    "        Embeds encoder parameters into the model weights using hash-based positioning.\n",
    "        \"\"\"\n",
    "        param_vector, layer_info = self._get_conv_params(model)\n",
    "        total_params = param_vector.numel()\n",
    "        watermark_size = encoder_vector.numel()\n",
    "\n",
    "        bitmap = torch.zeros(total_params, dtype=torch.bool, device=self.device)\n",
    "        embedded_positions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(min(watermark_size, total_params // 10)), desc=\"Embedding\"):\n",
    "                decoder_val = decoder_vector[i % len(decoder_vector)].item()\n",
    "                position = self._hash_position(decoder_val, i, total_params)\n",
    "\n",
    "                original_pos = position\n",
    "                while bitmap[position]:\n",
    "                    position = (position + 1) % total_params\n",
    "                    if position == original_pos:\n",
    "                        break\n",
    "\n",
    "                embedded_positions.append(position)\n",
    "                bitmap[position] = True\n",
    "\n",
    "\n",
    "        return embedded_positions, bitmap, layer_info\n",
    "\n",
    "    def _embed_params_into_model(self, model, encoder_vector, embedded_positions):\n",
    "        \"\"\"\n",
    "        Embeds encoder parameters into model weights at the specified positions.\n",
    "        Returns the modified model.\n",
    "        \"\"\"\n",
    "        # Build direct mapping: global_param_index -> encoder_value\n",
    "        position_to_encoder_value = {\n",
    "            pos: encoder_vector[i]\n",
    "            for i, pos in enumerate(embedded_positions)\n",
    "            if i < len(encoder_vector)\n",
    "        }\n",
    "\n",
    "        # Get model layers info\n",
    "        _, layers_info = self._get_conv_params(model)\n",
    "\n",
    "        # Embed into model\n",
    "        param_idx = 0\n",
    "        with torch.no_grad():\n",
    "            for info in layers_info:\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name == info['name']:\n",
    "                        param_flat = param.view(-1)\n",
    "                        for j in range(info['numel']):\n",
    "                            global_idx = param_idx + j\n",
    "                            if global_idx in position_to_encoder_value:\n",
    "                                param_flat[j] = position_to_encoder_value[global_idx]\n",
    "                        break\n",
    "                param_idx += info['numel']\n",
    "\n",
    "        return model\n",
    "    def _extract_and_evaluate(self, model, embedded_positions, decoder, dataloader):\n",
    "        \"\"\"\n",
    "        Extracts parameters from model at given positions,\n",
    "        reconstructs the encoder and full autoencoder,\n",
    "        evaluates MSE on dataloader.\n",
    "        Returns reconstructed autoencoder and avg MSE.\n",
    "        \"\"\"\n",
    "        # Get current model parameters\n",
    "        param_vector, _ = self._get_conv_params(model)\n",
    "\n",
    "        # Extract values at embedded positions\n",
    "        extracted_params = torch.zeros(len(embedded_positions), device=self.device)\n",
    "        with torch.no_grad():\n",
    "            for i, pos in enumerate(embedded_positions):\n",
    "                if pos < len(param_vector):\n",
    "                    extracted_params[i] = param_vector[pos]\n",
    "\n",
    "        # Reconstruct encoder by loading extracted values into architecture\n",
    "        reconstructed_encoder = HufuEncoder(\n",
    "            in_channels=self.config[\"in_channels\"],\n",
    "            latent_dim=self.config[\"latent_dim\"]\n",
    "        ).to(self.device)\n",
    "\n",
    "        offset = 0\n",
    "        with torch.no_grad():\n",
    "            for name, param in reconstructed_encoder.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    numel = param.numel()\n",
    "                    chunk = extracted_params[offset:offset + numel]\n",
    "                    if len(chunk) == numel:\n",
    "                        param.copy_(chunk.view(param.shape))\n",
    "                    offset += numel\n",
    "\n",
    "        # Rebuild full autoencoder with reconstructed encoder + owner decoder\n",
    "        reconstructed_autoencoder = HufuAutoencoder(\n",
    "            in_channels=self.config[\"in_channels\"],\n",
    "            latent_dim=self.config[\"latent_dim\"]\n",
    "        ).to(self.device)\n",
    "        reconstructed_autoencoder.encoder = reconstructed_encoder\n",
    "        reconstructed_autoencoder.decoder = decoder\n",
    "\n",
    "        # Evaluate MSE on dataloader\n",
    "        reconstructed_autoencoder.eval()\n",
    "        criterion = nn.MSELoss()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, _ in dataloader:\n",
    "                images = images.to(self.device)\n",
    "                images_norm = (images + 1) / 2\n",
    "                _, reconstructed = reconstructed_autoencoder(images_norm)\n",
    "                loss = criterion(reconstructed, images_norm)\n",
    "                total_loss += loss.item()\n",
    "                n_batches += 1\n",
    "                if n_batches >= 10:\n",
    "                    break\n",
    "\n",
    "        avg_mse = total_loss / n_batches\n",
    "        print(f\"Reconstructed autoencoder MSE: {avg_mse:.4f}\")\n",
    "\n",
    "        return reconstructed_autoencoder, avg_mse\n",
    "\n",
    "    def embed(self, dataloader):\n",
    "        \"\"\"\n",
    "        Embeds HufuNet encoder into DDPM model weights.\n",
    "        \"\"\"\n",
    "        print(f\"--- Demarrage Embedding HufuNet ---\")\n",
    "\n",
    "        autoencoder = HufuAutoencoder(\n",
    "            in_channels=self.config[\"in_channels\"],\n",
    "            latent_dim=self.config[\"latent_dim\"]\n",
    "        ).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "        # 1. Train autoencoder\n",
    "        autoencoder = self._train_autoencoder(autoencoder, dataloader, self.config[\"ae_epochs\"])\n",
    "        encoder = autoencoder.encoder\n",
    "        decoder = autoencoder.decoder\n",
    "\n",
    "        encoder_vector = self.get_encoder_parameters(encoder)\n",
    "        decoder_vector = self.get_decoder_parameters(decoder)\n",
    "        watermark_size = encoder_vector.numel()\n",
    "\n",
    "\n",
    "\n",
    "        # 4. Get model parameters\n",
    "        watermarked_model = self.model\n",
    "        embedded_positions, bitmap, layers_info=self.embedded_positons_in_model( watermarked_model, encoder_vector, decoder_vector)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 8. Fine-tune to maintain model quality\n",
    "        print(\"\\n--- Fine-tuning watermarked model ---\")\n",
    "        watermarked_model.train()\n",
    "        optimizer = torch.optim.AdamW(watermarked_model.parameters(), lr=self.config[\"lr\"])\n",
    "        mse_loss = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(self.config[\"epochs\"]):\n",
    "            encoder_vector = self.get_encoder_parameters(encoder)\n",
    "\n",
    "            watermarked_model=self._embed_params_into_model(watermarked_model, encoder_vector, embedded_positions)\n",
    "            pbar = tqdm(dataloader, desc=f\"Finetune Epoch {epoch+1}\")\n",
    "            for clean_images, _ in pbar:\n",
    "\n",
    "\n",
    "\n",
    "                clean_images = clean_images.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                recon, mu, logvar = watermarked_model(clean_images)\n",
    "\n",
    "                # Reconstruction loss\n",
    "                l_recon = F.mse_loss(recon, clean_images)\n",
    "\n",
    "                # KL divergence\n",
    "                l_kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "                # VAE total task loss\n",
    "                loss = l_recon + self.config[\"beta_kl\"] * l_kl\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.set_postfix(Loss=loss.item())\n",
    "            reconstructed_autoencoder, avg_mse=self._extract_and_evaluate(watermarked_model, embedded_positions, decoder, dataloader)\n",
    "            print(f\"Epoch {epoch+1} - Finetune Loss: {loss.item():.4f} | Extracted Autoencoder MSE: {avg_mse:.4f}\")\n",
    "            if avg_mse <  self.config[\"mse_threshold\"]:\n",
    "                print(f\"MSE= {avg_mse} is acceptable, stopping fine-tuning to preserve watermark integrity.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"MSE= {avg_mse} is too high, continuing fine-tuning to improve autoencoder reconstruction.\")\n",
    "                autoencoder = self._train_autoencoder(reconstructed_autoencoder, dataloader, self.config[\"ae_epochs\"])\n",
    "                encoder = autoencoder.encoder\n",
    "\n",
    "\n",
    "        # Save keys\n",
    "        self.saved_keys = {\n",
    "            \"autoencoder\": reconstructed_autoencoder,\n",
    "            \"encoder\": reconstructed_autoencoder.encoder,\n",
    "            \"decoder\": reconstructed_autoencoder.decoder,\n",
    "            # \"encoder_vector\": encoder_vector,\n",
    "            # \"decoder_vector\": decoder_vector,\n",
    "            \"embedded_positions\": embedded_positions,\n",
    "            \"layers_info\": layers_info,\n",
    "            \"watermarked_model\": watermarked_model,\n",
    "        }\n",
    "        torch.save(self.saved_keys, \"hufu_VAE_model_checkpoint.pt\")\n",
    "        return watermarked_model\n",
    "\n",
    "    # def extract(self, model=None):\n",
    "    #     \"\"\"\n",
    "    #     Extracts the embedded encoder from a suspect model.\n",
    "    #     \"\"\"\n",
    "    #     if model is None:\n",
    "    #         model = self.saved_keys[\"watermarked_model\"]\n",
    "    #\n",
    "    #     decoder_vector = self.saved_keys[\"decoder_vector\"]\n",
    "    #     embedded_positions = self.saved_keys[\"embedded_positions\"]\n",
    "    #     encoder_vector = self.saved_keys[\"encoder_vector\"]\n",
    "    #\n",
    "    #     # Get model parameters\n",
    "    #     param_vector, _ = self._get_conv_params(model)\n",
    "    #\n",
    "    #     # Extract encoder parameters\n",
    "    #     extracted_encoder = torch.zeros_like(encoder_vector[:len(embedded_positions)])\n",
    "    #\n",
    "    #     with torch.no_grad():\n",
    "    #         for i, pos in enumerate(embedded_positions):\n",
    "    #             if pos < len(param_vector) and i < len(extracted_encoder):\n",
    "    #                 extracted_encoder[i] = param_vector[pos]\n",
    "    #\n",
    "    #     # Compute correlation/similarity with original encoder\n",
    "    #     original_encoder = encoder_vector[:len(embedded_positions)].to(self.device)\n",
    "    #     extracted_encoder = extracted_encoder.to(self.device)\n",
    "    #\n",
    "    #     mse = F.mse_loss(extracted_encoder, original_encoder).item()\n",
    "    #     correlation = F.cosine_similarity(\n",
    "    #         extracted_encoder.unsqueeze(0),\n",
    "    #         original_encoder.unsqueeze(0)\n",
    "    #     ).item()\n",
    "    #\n",
    "    #     print(f\"Extraction MSE: {mse:.4f}\")\n",
    "    #     print(f\"Correlation with original: {correlation:.4f}\")\n",
    "    #\n",
    "    #     return mse, correlation, extracted_encoder\n",
    "\n",
    "\n",
    "    def extract(self, model=None, dataloader=None):\n",
    "        \"\"\"\n",
    "        Extracts the embedded encoder from a suspect model and tests its functionality\n",
    "        by reconstructing images using the owner's decoder.\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            model = self.saved_keys[\"watermarked_model\"]\n",
    "\n",
    "        decoder = self.saved_keys[\"decoder\"]\n",
    "        embedded_positions = self.saved_keys[\"embedded_positions\"]\n",
    "\n",
    "\n",
    "\n",
    "        reconstructed_autoencoder, avg_mse=self._extract_and_evaluate(model, embedded_positions, decoder, dataloader)\n",
    "        extracted_encoder = reconstructed_autoencoder.encoder\n",
    "\n",
    "\n",
    "        return avg_mse, extracted_encoder\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T10:25:00.254032698Z",
     "start_time": "2026-02-18T10:25:00.214906899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load from Hugging Face (no Google Drive issues)\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "hf_dataset = load_from_disk(\"celeba_local\")\n",
    "# os.makedirs(\"./celeba_images/all\", exist_ok=True)\n",
    "# for i, item in enumerate(hf_dataset):\n",
    "#     item['image'].save(f\"./celeba_images/all/{i:06d}.jpg\")\n",
    "# del hf_dataset\n",
    "\n",
    "\n",
    "class CelebAWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, transform):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0\n",
    "\n",
    "dataset = CelebAWrapper(hf_dataset, transform)\n",
    "# del hf_dataset\n",
    "# gc.collect()\n",
    "# dataset = datasets.ImageFolder(\"./celeba_images\", transform=transform)\n",
    "print(\"Dataset loaded!\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "print(\"loader loaded!\")\n"
   ],
   "id": "dba9265cb5b5c4eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded!\n",
      "loader loaded!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "embedding_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T11:25:48.135959940Z",
     "start_time": "2026-02-18T10:25:01.047331024Z"
    }
   },
   "source": [
    "# --- EXEMPLE D'EXECUTION ---\n",
    "#load model\n",
    "latent_dim = 200\n",
    "\n",
    "# Initialize the model\n",
    "model = VAE(latent_dim=latent_dim)\n",
    "\n",
    "# Load the trained weights\n",
    "model_path = \"./vae_celeba_latent_200_epochs_10_batch_64_subset_80000.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "\n",
    "# 2. Embedding HufuNet\n",
    "hufu_defense = HufuVAE(model=model, device=device)\n",
    "watermarked_model = hufu_defense.embed(dataloader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Demarrage Embedding HufuNet ---\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:27<00:00, 36.06it/s, Loss=0.00234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 37728/37728 [00:00<00:00, 51254.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuning watermarked model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 1: 100%|██████████| 3166/3166 [02:02<00:00, 25.82it/s, Loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0625\n",
      "Epoch 1 - Finetune Loss: 0.3376 | Extracted Autoencoder MSE: 0.0625\n",
      "MSE= 0.06253124102950096 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:18<00:00, 40.42it/s, Loss=0.00157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 2: 100%|██████████| 3166/3166 [01:24<00:00, 37.49it/s, Loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1341\n",
      "Epoch 2 - Finetune Loss: 0.3161 | Extracted Autoencoder MSE: 0.1341\n",
      "MSE= 0.13408446907997132 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:17<00:00, 40.63it/s, Loss=0.00125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 3: 100%|██████████| 3166/3166 [01:23<00:00, 37.92it/s, Loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0607\n",
      "Epoch 3 - Finetune Loss: 0.2969 | Extracted Autoencoder MSE: 0.0607\n",
      "MSE= 0.06066616475582123 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:19<00:00, 40.07it/s, Loss=0.00103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 4: 100%|██████████| 3166/3166 [01:23<00:00, 37.80it/s, Loss=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0347\n",
      "Epoch 4 - Finetune Loss: 0.3184 | Extracted Autoencoder MSE: 0.0347\n",
      "MSE= 0.03467031754553318 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:18<00:00, 40.17it/s, Loss=0.00102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 5: 100%|██████████| 3166/3166 [01:24<00:00, 37.67it/s, Loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0599\n",
      "Epoch 5 - Finetune Loss: 0.2612 | Extracted Autoencoder MSE: 0.0599\n",
      "MSE= 0.05985407680273056 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:19<00:00, 39.92it/s, Loss=0.000853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 6: 100%|██████████| 3166/3166 [01:24<00:00, 37.67it/s, Loss=0.319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0734\n",
      "Epoch 6 - Finetune Loss: 0.3189 | Extracted Autoencoder MSE: 0.0734\n",
      "MSE= 0.07336710318922997 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:17<00:00, 40.66it/s, Loss=0.000881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 7: 100%|██████████| 3166/3166 [01:23<00:00, 37.96it/s, Loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0238\n",
      "Epoch 7 - Finetune Loss: 0.3318 | Extracted Autoencoder MSE: 0.0238\n",
      "MSE= 0.02378745935857296 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:19<00:00, 39.75it/s, Loss=0.000972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 8: 100%|██████████| 3166/3166 [01:25<00:00, 37.14it/s, Loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0581\n",
      "Epoch 8 - Finetune Loss: 0.3263 | Extracted Autoencoder MSE: 0.0581\n",
      "MSE= 0.0580789964646101 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:20<00:00, 39.52it/s, Loss=0.000754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 9: 100%|██████████| 3166/3166 [01:24<00:00, 37.29it/s, Loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0447\n",
      "Epoch 9 - Finetune Loss: 0.2907 | Extracted Autoencoder MSE: 0.0447\n",
      "MSE= 0.04472903162240982 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:18<00:00, 40.08it/s, Loss=0.000718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 10: 100%|██████████| 3166/3166 [01:25<00:00, 37.17it/s, Loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0123\n",
      "Epoch 10 - Finetune Loss: 0.2927 | Extracted Autoencoder MSE: 0.0123\n",
      "MSE= 0.012346486002206803 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:20<00:00, 39.25it/s, Loss=0.000715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 11: 100%|██████████| 3166/3166 [01:24<00:00, 37.53it/s, Loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0243\n",
      "Epoch 11 - Finetune Loss: 0.3302 | Extracted Autoencoder MSE: 0.0243\n",
      "MSE= 0.0242686090990901 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:18<00:00, 40.11it/s, Loss=0.000733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 12: 100%|██████████| 3166/3166 [01:25<00:00, 37.20it/s, Loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0204\n",
      "Epoch 12 - Finetune Loss: 0.2976 | Extracted Autoencoder MSE: 0.0204\n",
      "MSE= 0.02036541383713484 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:20<00:00, 39.52it/s, Loss=0.000713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 13: 100%|██████████| 3166/3166 [01:25<00:00, 37.17it/s, Loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0308\n",
      "Epoch 13 - Finetune Loss: 0.3074 | Extracted Autoencoder MSE: 0.0308\n",
      "MSE= 0.030797526985406876 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:19<00:00, 39.65it/s, Loss=0.00066] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 14: 100%|██████████| 3166/3166 [01:25<00:00, 37.15it/s, Loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0225\n",
      "Epoch 14 - Finetune Loss: 0.2948 | Extracted Autoencoder MSE: 0.0225\n",
      "MSE= 0.0224681967869401 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:18<00:00, 40.16it/s, Loss=0.000699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 15: 100%|██████████| 3166/3166 [01:23<00:00, 38.00it/s, Loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0250\n",
      "Epoch 15 - Finetune Loss: 0.2926 | Extracted Autoencoder MSE: 0.0250\n",
      "MSE= 0.025029807537794112 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:18<00:00, 40.43it/s, Loss=0.000672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 16: 100%|██████████| 3166/3166 [01:23<00:00, 37.88it/s, Loss=0.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0287\n",
      "Epoch 16 - Finetune Loss: 0.3005 | Extracted Autoencoder MSE: 0.0287\n",
      "MSE= 0.02872729189693928 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:19<00:00, 39.74it/s, Loss=0.000632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 17: 100%|██████████| 3166/3166 [01:22<00:00, 38.27it/s, Loss=0.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0499\n",
      "Epoch 17 - Finetune Loss: 0.3499 | Extracted Autoencoder MSE: 0.0499\n",
      "MSE= 0.0498591635376215 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:17<00:00, 40.87it/s, Loss=0.000712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 18: 100%|██████████| 3166/3166 [01:23<00:00, 37.91it/s, Loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0103\n",
      "Epoch 18 - Finetune Loss: 0.3271 | Extracted Autoencoder MSE: 0.0103\n",
      "MSE= 0.010330616123974323 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:18<00:00, 40.08it/s, Loss=0.000583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 19: 100%|██████████| 3166/3166 [01:23<00:00, 38.11it/s, Loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0960\n",
      "Epoch 19 - Finetune Loss: 0.3097 | Extracted Autoencoder MSE: 0.0960\n",
      "MSE= 0.09601198583841324 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:17<00:00, 40.88it/s, Loss=0.000677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 20: 100%|██████████| 3166/3166 [01:22<00:00, 38.15it/s, Loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0129\n",
      "Epoch 20 - Finetune Loss: 0.3043 | Extracted Autoencoder MSE: 0.0129\n",
      "MSE= 0.012871145736426115 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:19<00:00, 40.01it/s, Loss=0.000835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 21: 100%|██████████| 3166/3166 [01:23<00:00, 37.75it/s, Loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0174\n",
      "Epoch 21 - Finetune Loss: 0.2829 | Extracted Autoencoder MSE: 0.0174\n",
      "MSE= 0.01736978329718113 is too high, continuing fine-tuning to improve autoencoder reconstruction.\n",
      "--- Pre-training HufuNet Autoencoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/1: 100%|██████████| 3166/3166 [01:17<00:00, 41.04it/s, Loss=0.000688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetune Epoch 22: 100%|██████████| 3166/3166 [01:22<00:00, 38.19it/s, Loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.0084\n",
      "Epoch 22 - Finetune Loss: 0.2886 | Extracted Autoencoder MSE: 0.0084\n",
      "MSE= 0.008433844894170761 is acceptable, stopping fine-tuning to preserve watermark integrity.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "distillation_attack_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T12:07:03.864590376Z",
     "start_time": "2026-02-18T12:07:03.805428054Z"
    }
   },
   "source": [
    "# --- Fonction de Distillation (Attaque) ---\n",
    "\n",
    "def run_distillation_attack_hufu(hufu_obj, dataloader, epochs=5, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Tente de transferer la fonctionnalite du modele HufuNet vers un modele vierge.\n",
    "    Verifie si le watermark survit.\n",
    "    \"\"\"\n",
    "    device = hufu_obj.device\n",
    "    checkpoint = torch.load(\"hufu_VAE_model_checkpoint.pt\", weights_only=False)\n",
    "\n",
    "    # 1. Teacher (Gele)\n",
    "    # teacher = checkpoint[\"watermarked_model\"]\n",
    "    teacher=hufu_obj.saved_keys[\"watermarked_model\"]\n",
    "    teacher.eval()\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 2. Student (Vierge - Meme architecture)\n",
    "    print(\"\\n--- Initialisation du Student ---\")\n",
    "    student=VAE(latent_dim=latent_dim)\n",
    "    student.load_state_dict(torch.load(model_path))\n",
    "    student.to(device)\n",
    "    student.train()\n",
    "\n",
    "\n",
    "    mse_teacher, _ = hufu_obj.extract(teacher, dataloader)\n",
    "    print(\"\\n[Check] Teacher: mse_hufu_teacher: {:.4f}\".format(mse_teacher))\n",
    "\n",
    "    mse_student, _ = hufu_obj.extract(student, dataloader)\n",
    "    print(\"\\n[Check] Student (before): mse_hufu_student: {:.4f}\".format(mse_student))\n",
    "\n",
    "    optimizer = AdamW(student.parameters(), lr=lr)\n",
    "\n",
    "    history = {\"loss\": [], \"MSE\": []}\n",
    "\n",
    "    print(f\"\\n--- Distillation HufuNet ({epochs} epochs) ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for clean_images, _ in pbar:\n",
    "            clean_images = clean_images.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                target_pred,_,_ = teacher(clean_images)\n",
    "\n",
    "            student_pred,_,_ = student(clean_images)\n",
    "\n",
    "            loss = F.mse_loss(student_pred, target_pred)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix(Loss=loss.item())\n",
    "\n",
    "        # Check correlation\n",
    "        avg_mse, _ = hufu_obj.extract(student, dataloader)\n",
    "        history[\"MSE\"].append(avg_mse)\n",
    "        history[\"loss\"].append(running_loss / len(dataloader))\n",
    "\n",
    "        print(f\"Fin Epoch {epoch+1} | Loss: {history['loss'][-1]:.4f} | MSE_hufu_autoencoder: {avg_mse:.4f}\")\n",
    "\n",
    "\n",
    "    return student, history\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "run_attack_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T12:46:13.943445632Z",
     "start_time": "2026-02-18T12:07:04.450502805Z"
    }
   },
   "source": [
    "# 5. Attaque par Distillation\n",
    "student_res, stats = run_distillation_attack_hufu(hufu_defense, dataloader, epochs=100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initialisation du Student ---\n",
      "Reconstructed autoencoder MSE: 0.0234\n",
      "\n",
      "[Check] Teacher: mse_hufu_teacher: 0.0234\n",
      "Reconstructed autoencoder MSE: 0.1601\n",
      "\n",
      "[Check] Student (before): mse_hufu_student: 0.1601\n",
      "\n",
      "--- Distillation HufuNet (100 epochs) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3166/3166 [01:25<00:00, 37.04it/s, Loss=0.00486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1556\n",
      "Fin Epoch 1 | Loss: 0.0082 | MSE_hufu_autoencoder: 0.1556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3166/3166 [01:25<00:00, 37.05it/s, Loss=0.00706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1550\n",
      "Fin Epoch 2 | Loss: 0.0069 | MSE_hufu_autoencoder: 0.1550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3166/3166 [01:25<00:00, 36.93it/s, Loss=0.0078] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1602\n",
      "Fin Epoch 3 | Loss: 0.0068 | MSE_hufu_autoencoder: 0.1602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3166/3166 [01:25<00:00, 36.85it/s, Loss=0.00752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1618\n",
      "Fin Epoch 4 | Loss: 0.0067 | MSE_hufu_autoencoder: 0.1618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 3166/3166 [01:24<00:00, 37.36it/s, Loss=0.00602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1595\n",
      "Fin Epoch 5 | Loss: 0.0067 | MSE_hufu_autoencoder: 0.1595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 3166/3166 [01:24<00:00, 37.41it/s, Loss=0.00702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1596\n",
      "Fin Epoch 6 | Loss: 0.0067 | MSE_hufu_autoencoder: 0.1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 3166/3166 [01:25<00:00, 37.13it/s, Loss=0.00697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1611\n",
      "Fin Epoch 7 | Loss: 0.0067 | MSE_hufu_autoencoder: 0.1611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 3166/3166 [01:24<00:00, 37.39it/s, Loss=0.00889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1605\n",
      "Fin Epoch 8 | Loss: 0.0067 | MSE_hufu_autoencoder: 0.1605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3166/3166 [01:24<00:00, 37.57it/s, Loss=0.00736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1604\n",
      "Fin Epoch 9 | Loss: 0.0067 | MSE_hufu_autoencoder: 0.1604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 3166/3166 [01:24<00:00, 37.45it/s, Loss=0.00516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1532\n",
      "Fin Epoch 10 | Loss: 0.0067 | MSE_hufu_autoencoder: 0.1532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 3166/3166 [01:24<00:00, 37.44it/s, Loss=0.00696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1586\n",
      "Fin Epoch 11 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 3166/3166 [01:24<00:00, 37.36it/s, Loss=0.00536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1522\n",
      "Fin Epoch 12 | Loss: 0.0067 | MSE_hufu_autoencoder: 0.1522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 3166/3166 [01:30<00:00, 35.14it/s, Loss=0.00632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1598\n",
      "Fin Epoch 13 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 3166/3166 [01:23<00:00, 37.78it/s, Loss=0.00672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1541\n",
      "Fin Epoch 14 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 3166/3166 [01:24<00:00, 37.52it/s, Loss=0.00448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1583\n",
      "Fin Epoch 15 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 3166/3166 [01:23<00:00, 37.74it/s, Loss=0.00487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1600\n",
      "Fin Epoch 16 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 3166/3166 [01:27<00:00, 36.15it/s, Loss=0.00637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1520\n",
      "Fin Epoch 17 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 3166/3166 [01:25<00:00, 37.06it/s, Loss=0.00714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1587\n",
      "Fin Epoch 18 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 3166/3166 [01:25<00:00, 36.96it/s, Loss=0.00779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1617\n",
      "Fin Epoch 19 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 3166/3166 [01:25<00:00, 37.18it/s, Loss=0.00836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1579\n",
      "Fin Epoch 20 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 3166/3166 [01:25<00:00, 37.13it/s, Loss=0.00701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1630\n",
      "Fin Epoch 21 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 3166/3166 [01:24<00:00, 37.31it/s, Loss=0.00742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1630\n",
      "Fin Epoch 22 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 3166/3166 [01:25<00:00, 37.18it/s, Loss=0.00892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1551\n",
      "Fin Epoch 23 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 3166/3166 [01:24<00:00, 37.25it/s, Loss=0.00555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1635\n",
      "Fin Epoch 24 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 3166/3166 [01:25<00:00, 37.09it/s, Loss=0.00606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1652\n",
      "Fin Epoch 25 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 3166/3166 [01:25<00:00, 37.12it/s, Loss=0.00696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1588\n",
      "Fin Epoch 26 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 3166/3166 [01:23<00:00, 37.78it/s, Loss=0.00518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed autoencoder MSE: 0.1703\n",
      "Fin Epoch 27 | Loss: 0.0066 | MSE_hufu_autoencoder: 0.1703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28:  45%|████▌     | 1429/3166 [00:37<00:45, 38.06it/s, Loss=0.00679]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# 5. Attaque par Distillation\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m student_res, stats = \u001B[43mrun_distillation_attack_hufu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhufu_defense\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 42\u001B[39m, in \u001B[36mrun_distillation_attack_hufu\u001B[39m\u001B[34m(hufu_obj, dataloader, epochs, lr)\u001B[39m\n\u001B[32m     39\u001B[39m pbar = tqdm(dataloader, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     40\u001B[39m running_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclean_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpbar\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m    \u001B[49m\u001B[43mclean_images\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mclean_images\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mno_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    729\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    730\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    731\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    734\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    735\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    736\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    738\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    786\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    787\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m788\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    789\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    790\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 41\u001B[39m, in \u001B[36mCelebAWrapper.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m     39\u001B[39m image = \u001B[38;5;28mself\u001B[39m.dataset[idx][\u001B[33m'\u001B[39m\u001B[33mimage\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform:\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     image = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m image, \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001B[39m, in \u001B[36mToTensor.__call__\u001B[39m\u001B[34m(self, pic)\u001B[39m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[32m    130\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    131\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    135\u001B[39m \u001B[33;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[32m    136\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/WatDNN/venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:174\u001B[39m, in \u001B[36mto_tensor\u001B[39m\u001B[34m(pic)\u001B[39m\n\u001B[32m    172\u001B[39m img = img.view(pic.size[\u001B[32m1\u001B[39m], pic.size[\u001B[32m0\u001B[39m], F_pil.get_image_num_channels(pic))\n\u001B[32m    173\u001B[39m \u001B[38;5;66;03m# put it from HWC to CHW format\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m174\u001B[39m img = \u001B[43mimg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcontiguous\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch.ByteTensor):\n\u001B[32m    176\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img.to(dtype=default_float_dtype).div(\u001B[32m255\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "plot_results_cell",
   "metadata": {},
   "source": [
    "# 6. Visualisation des resultats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(stats[\"loss\"])\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Distillation Loss\")\n",
    "\n",
    "ax2.plot(stats[\"correlation\"])\n",
    "ax2.axhline(y=0.5, color='r', linestyle='--', label='Threshold (0.5)')\n",
    "ax2.axhline(y=0.0, color='g', linestyle='--', label='No correlation (0.0)')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Correlation\")\n",
    "ax2.set_title(\"Encoder Correlation During Distillation\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "visualize_ae_cell",
   "metadata": {},
   "source": [
    "# 7. Visualize Autoencoder Reconstruction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "autoencoder = hufu_defense.saved_keys[\"autoencoder\"]\n",
    "autoencoder.eval()\n",
    "\n",
    "# Get some test images\n",
    "test_images, _ = next(iter(dataloader))\n",
    "test_images = test_images[:8].to(hufu_defense.device)\n",
    "test_images_norm = (test_images + 1) / 2\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, reconstructed = autoencoder(test_images_norm)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    img = test_images_norm[i].cpu().permute(1, 2, 0).numpy()\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original')\n",
    "\n",
    "    # Reconstructed\n",
    "    rec = reconstructed[i].cpu().permute(1, 2, 0).numpy()\n",
    "    axes[1, i].imshow(rec)\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Reconstructed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
